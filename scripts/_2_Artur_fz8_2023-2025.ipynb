{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab32df8",
   "metadata": {},
   "source": [
    "# FZ8 PREP (2023 - 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d15f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProcess all “regular” FZ-8 workbooks (2023-2025) and build the final,\\nclean data sets used downstream.\\n\\nOverview\\n========\\nUnlike the legacy 2020-2022 file (single PDF-style workbook), the\\n2023-2025 data arrive month-by-month as normal Excel files\\n`fz8_YYYYMM.xlsx`.  \\nFor every file we\\n\\n1. **Detect & parse** sheets 8.2, 8.3, 8.6, 8.7, 8.8, 8.9 and 8.16  \\n   (sheet 8.1 no longer exists in this era).\\n2. **Clean** each sheet with a dedicated parser (remove totals, tidy\\n   headers, convert NaN → “”, etc.).\\n3. **Append** a `Date` column (the YYYYMM extracted from the filename).\\n4. **Accumulate** rows per sheet across all months.\\n5. **Export** one “raw” CSV per sheet for 2023-2025:\\n\\n       ../data/raw/fz8/csv/fz_8.<N>_2023-2025_raw.csv\\n\\n6. **Validate**: compare the new headers with those from the 2020-2022\\n   period; if they match 1:1, concatenate the two eras.\\n7. **Save** the final, merged files:\\n\\n       ../data/processed/fz_8.<N>_raw.csv\\n\\nFolder layout\\n-------------\\n* `../data/raw/fz8/`        monthly Excel workbooks  \\n* `../data/raw/fz8/csv/`    intermediate “raw” CSVs (both eras)  \\n* `../data/processed/`      final merged CSVs for analytics\\n\\nKey points\\n----------\\n* **Robust sheet lookup** – tolerates “FZ 8.2” *and* “FZ8.2”.\\n* **Strict header check** – prevents silent schema drift.\\n* **All cells are stored as text** (`dtype=str`), no numeric coercion.\\n* **Console log** prints a concise ✓ for each successful step and a entry for any discrepancies.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process all “regular” FZ-8 workbooks (2023-2025) and build the final,\n",
    "clean data sets used downstream.\n",
    "\n",
    "Overview\n",
    "========\n",
    "Unlike the legacy 2020-2022 file (single PDF-style workbook), the\n",
    "2023-2025 data arrive month-by-month as normal Excel files\n",
    "`fz8_YYYYMM.xlsx`.  \n",
    "For every file we\n",
    "\n",
    "1. **Detect & parse** sheets 8.2, 8.3, 8.6, 8.7, 8.8, 8.9 and 8.16  \n",
    "   (sheet 8.1 no longer exists in this era).\n",
    "2. **Clean** each sheet with a dedicated parser (remove totals, tidy\n",
    "   headers, convert NaN → “”, etc.).\n",
    "3. **Append** a `Date` column (the YYYYMM extracted from the filename).\n",
    "4. **Accumulate** rows per sheet across all months.\n",
    "5. **Export** one “raw” CSV per sheet for 2023-2025:\n",
    "\n",
    "       ../data/raw/fz8/csv/fz_8.<N>_2023-2025_raw.csv\n",
    "\n",
    "6. **Validate**: compare the new headers with those from the 2020-2022\n",
    "   period; if they match 1:1, concatenate the two eras.\n",
    "7. **Save** the final, merged files:\n",
    "\n",
    "       ../data/processed/fz_8.<N>_raw.csv\n",
    "\n",
    "Folder layout\n",
    "-------------\n",
    "* `../data/raw/fz8/`        monthly Excel workbooks  \n",
    "* `../data/raw/fz8/csv/`    intermediate “raw” CSVs (both eras)  \n",
    "* `../data/processed/`      final merged CSVs for analytics\n",
    "\n",
    "Key points\n",
    "----------\n",
    "* **Robust sheet lookup** – tolerates “FZ 8.2” *and* “FZ8.2”.\n",
    "* **Strict header check** – prevents silent schema drift.\n",
    "* **All cells are stored as text** (`dtype=str`), no numeric coercion.\n",
    "* **Console log** prints a concise ✓ for each successful step and a entry for any discrepancies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46a6c9",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  IMPORTS & PATHS  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1daf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DATA_DIR = Path(\"../data/raw/fz8\")          # input *.xlsx files\n",
    "OUT_DIR  = Path(\"../data/raw/fz8/csv\")      # will hold *_raw.csv\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DST_DIR = Path(\"../data/processed/,\")         # final merged CSVs\n",
    "DST_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db963102",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  HELPERS  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c60ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _date_from_fname(p):\n",
    "    \"\"\"Return YYYYMM extracted from filename `fz8_YYYYMM.xlsx`.\"\"\"\n",
    "    return re.search(r\"(\\d{6})\", p.name).group(1)\n",
    "\n",
    "def _col(ws, letter, r0, r1):\n",
    "    \"\"\"Return values of column *letter* from rows *r0 … r1* inclusive.\"\"\"\n",
    "    return [ws[f\"{letter}{row}\"].value for row in range(r0, r1 + 1)]\n",
    "\n",
    "# def _clean_header(s):\n",
    "#     \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "#     return str(s).replace('\\n', ' ').replace('  ', ' ').strip().upper() if s is not None else s\n",
    "\n",
    "def _clean_header(s):\n",
    "    \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "    return (str(s).translate(str.maketrans(\"äÄöÖüÜ\", \"aAoOuU\")).replace(\"\\n\", \" \").replace(\"  \", \" \").strip().upper()) if s is not None else s\n",
    "\n",
    "def _strip_cols(df):\n",
    "    \"\"\"Apply `clean_header` to every column name in-place and return df.\"\"\"\n",
    "    df.columns = [_clean_header(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique(cols):\n",
    "    \"\"\"Ensure uniqueness by adding numeric suffixes.\"\"\"\n",
    "    seen, out = {}, []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}{seen[c]}\")\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def _find_sheet(wb, num):\n",
    "    \"\"\"Locate sheet whose name matches *pattern* (case-insensitive).\"\"\"\n",
    "    pattern = re.compile(fr\"^FZ\\s*8\\.{re.escape(num)}$\", flags=re.IGNORECASE)\n",
    "    for name in wb.sheetnames:\n",
    "        if pattern.match(name.strip()):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# def _strip_upper(df):\n",
    "#     \"\"\"\n",
    "#     Trim whitespace and up-case every object column (in-place).\n",
    "#     Faster & future-proof replacement for the old `applymap`.\n",
    "#     \"\"\"\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype == \"object\":\n",
    "#             df[col] = df[col].str.strip().str.upper()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749636f7",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  PARSERS  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c352b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEach fz8_X returns a cleaned DataFrame for one sheet.\\nOnly key points differ, so code is condensed for brevity.\\nIf you need to adjust one sheet,\\nchange the corresponding function only.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Each fz8_X returns a cleaned DataFrame for one sheet.\n",
    "Only key points differ, so code is condensed for brevity.\n",
    "If you need to adjust one sheet,\n",
    "change the corresponding function only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b67130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_1(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.1”** – monthly total registrations by make.\n",
    "\n",
    "    Cleaning workflow\n",
    "    -----------------\n",
    "    1. Header texts are normalised with `_clean_header()` and passed\n",
    "       through `_unique()` to guarantee column-name uniqueness.\n",
    "    2. Build DataFrame from the fixed cell ranges; drop rows that are\n",
    "       completely empty (`dropna(how=\"all\")`).\n",
    "    3. `_strip_cols()` runs again for idempotent safety.\n",
    "    4. Remove any row whose *Marke* column contains one of  \n",
    "       “INSGESAMT, FLENSBURG, HINWEIS, UMBENANNT”.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Worksheet instance for tab “FZ 8.1”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Clean table ready for vertical concatenation across months.\n",
    "    \"\"\"\n",
    "    \n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),  # Marke\n",
    "        _clean_header(ws[\"C9\"].value),  # Anzahl\n",
    "    ]\n",
    "    cols = _unique(raw)                 # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]: _col(ws, \"B\", 10, 100),\n",
    "        cols[1]: _col(ws, \"C\", 10, 100),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|FLENSBURG|HINWEIS|UMBENANNT\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a3c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_2(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.2”** – environmental breakdown by make.\n",
    "\n",
    "    Cleaning workflow\n",
    "    --------------\n",
    "    1. `_clean_header()` trims new-lines and duplicate spaces; `_unique()`\n",
    "       appends numeric suffixes to duplicate titles (if any).\n",
    "    2. Read the fixed cell ranges into a DataFrame; remove fully-empty\n",
    "       rows (`dropna(how=\"all\")`).\n",
    "    3. Run `_strip_cols()` (idempotent but keeps headers tidy).\n",
    "    4. Drop rows whose *Marke* column (first column) contains one of\n",
    "       “INSGESAMT, FLENSBURG, HINWEIS, UMBENANNT”.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        The worksheet object for tab “FZ 8.2”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Clean table, ready to be concatenated with other months.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),  # Marke\n",
    "        _clean_header(ws[\"C8\"].value),  # Anzahl\n",
    "        _clean_header(ws[\"D8\"].value),  # CO2-Emission in g/km\n",
    "        _clean_header(ws[\"F9\"].value),  # Euro 6\n",
    "        _clean_header(ws[\"I9\"].value),  # Elektro (BEV)\n",
    "        _clean_header(ws[\"J9\"].value),  # Hybrid\n",
    "        _clean_header(ws[\"K10\"].value), # darunter Plug-in\n",
    "    ]\n",
    "    cols = _unique(raw)                 # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]: _col(ws, \"B\", 11, 100),\n",
    "        cols[1]: _col(ws, \"C\", 11, 100),\n",
    "        cols[2]: _col(ws, \"D\", 11, 100),\n",
    "        cols[3]: _col(ws, \"F\", 11, 100),\n",
    "        cols[4]: _col(ws, \"I\", 11, 100),\n",
    "        cols[5]: _col(ws, \"J\", 11, 100),\n",
    "        cols[6]: _col(ws, \"K\", 11, 100),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|FLENSBURG|HINWEIS|UMBENANNT\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4d6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_3(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.3”** – drive-train mix by vehicle segment and model line.\n",
    "\n",
    "    Cleaning workflow\n",
    "    -----------------\n",
    "    1. _Header normalisation_ – `_clean_header()` trims new-lines /\n",
    "       double spaces; `_unique()` appends numeric suffixes if duplicates.\n",
    "    2. Build DataFrame from column slices; drop rows that are completely\n",
    "       empty (`dropna(how=\"all\")`).\n",
    "    3. `_strip_cols()`: idempotent header clean-up.\n",
    "    4. Forward-fill **Segment** column so sub-rows inherit their segment.\n",
    "    5. Remove meta / total rows whose *segment* column contains any of  \n",
    "       “ZUSAMMEN, INSGESAMT, HINWEISE, AUSGEWIESEN, KRAFTSTOFFVERBRAUCH,  \n",
    "        FLENSBURG, UMBENANNT”.\n",
    "    6. If a segment row is marked “SONSTIGE”, force *Modellreihe* to the\n",
    "       literal `\"SONSTIGE\"` for clarity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Worksheet handle for sheet “FZ 8.3”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Clean table, ready to be concatenated across months.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),   # Segment\n",
    "        _clean_header(ws[\"C8\"].value),   # Modellreihe\n",
    "        _clean_header(ws[\"D8\"].value),   # Insgesamt\n",
    "        _clean_header(ws[\"E10\"].value),  # CO2-Emission in g/km\n",
    "        _clean_header(ws[\"F9\"].value),   # Benzin\n",
    "        _clean_header(ws[\"G10\"].value),  # CO2-Emission in g/km\n",
    "        _clean_header(ws[\"H10\"].value),  # Kraftstoffverbrauch in l/100 km\n",
    "        _clean_header(ws[\"I9\"].value),   # Diesel\n",
    "        _clean_header(ws[\"J10\"].value),  # CO2-Emission in g/km\n",
    "        _clean_header(ws[\"K10\"].value),  # Kraftstoffverbrauch in l/100 km\n",
    "        _clean_header(ws[\"L9\"].value),   # Erdgas (CNG) (einschl. bivalent)\n",
    "        _clean_header(ws[\"M9\"].value),   # Flüssiggas (LPG) (einschl. bivalent)\n",
    "        _clean_header(ws[\"N9\"].value),   # Hybrid\n",
    "        _clean_header(ws[\"O10\"].value),  # dar. Plug-in\n",
    "        _clean_header(ws[\"P9\"].value),   # Elektro (BEV)\n",
    "    ]\n",
    "    cols = _unique(raw)                  # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 11, 500),\n",
    "        cols[1]:  _col(ws, \"C\", 11, 500),\n",
    "        cols[2]:  _col(ws, \"D\", 11, 500),\n",
    "        cols[3]:  _col(ws, \"E\", 11, 500),\n",
    "        cols[4]:  _col(ws, \"F\", 11, 500),\n",
    "        cols[5]:  _col(ws, \"G\", 11, 500),\n",
    "        cols[6]:  _col(ws, \"H\", 11, 500),\n",
    "        cols[7]:  _col(ws, \"I\", 11, 500),\n",
    "        cols[8]:  _col(ws, \"J\", 11, 500),\n",
    "        cols[9]:  _col(ws, \"K\", 11, 500),\n",
    "        cols[10]: _col(ws, \"L\", 11, 500),\n",
    "        cols[11]: _col(ws, \"M\", 11, 500),\n",
    "        cols[12]: _col(ws, \"N\", 11, 500),\n",
    "        cols[13]: _col(ws, \"O\", 11, 500),\n",
    "        cols[14]: _col(ws, \"P\", 11, 500),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    \n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # forward-fill Segment\n",
    "    seg_col = next((c for c in df.columns if \"segment\" in str(c).lower()), df.columns[0])\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"ZUSAMMEN|INSGESAMT|HINWEISE|AUSGEWIESEN|KRAFTSTOFFVERBRAUCH|FLENSBURG|HINWEIS|UMBENANNT\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    # flag “SONSTIGE” rows\n",
    "    mod_col = next((c for c in df.columns if \"modellreihe\" in str(c).lower()), None)\n",
    "    if mod_col:\n",
    "        is_sonstige = df[seg_col].astype(str).str.contains(r\"\\bSONSTIGE\\b\", case=False, na=False)\n",
    "        df.loc[is_sonstige, mod_col] = \"SONSTIGE\"\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[2:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bedafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_6(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.6”** – power-train mix by federal state.\n",
    "\n",
    "    Cleaning workflow\n",
    "    -----------------\n",
    "    1. Header texts are normalised via `_clean_header()`; duplicates get a\n",
    "       numeric suffix via `_unique()`.\n",
    "    2. Read the fixed range into a `DataFrame`; drop fully empty rows.\n",
    "    3. Run `_strip_cols()` (idempotent header tidy-up).\n",
    "    4. Remove any row whose first column contains one of the keywords  \n",
    "       “FLENSBURG, HINWEIS, UMBENANNT”.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Worksheet object for tab “FZ 8.6”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Clean table ready for monthly concatenation.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),  # Bundesland\n",
    "        _clean_header(ws[\"C8\"].value),  # Benzin insgesamt\n",
    "        _clean_header(ws[\"D8\"].value),  # Darunter Euro 6\n",
    "        _clean_header(ws[\"G8\"].value),  # Diesel insgesamt\n",
    "        _clean_header(ws[\"H8\"].value),  # Darunter Euro 6\n",
    "        _clean_header(ws[\"K8\"].value),  # Flüssiggas (LPG)  (einschl. bivalent)\n",
    "        _clean_header(ws[\"L8\"].value),  # Erdgas (CNG) (einschl. bivalent)\n",
    "        _clean_header(ws[\"M8\"].value),  # Elektro (BEV)\n",
    "        _clean_header(ws[\"N8\"].value),  # Hybrid\n",
    "        _clean_header(ws[\"O9\"].value),  # darunter Plug-in\n",
    "        _clean_header(ws[\"P8\"].value),  # Sonstige\n",
    "    ]\n",
    "    cols = _unique(raw)                 # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 10, 30),\n",
    "        cols[1]:  _col(ws, \"C\", 10, 30),\n",
    "        cols[2]:  _col(ws, \"D\", 10, 30),\n",
    "        cols[3]:  _col(ws, \"G\", 10, 30),\n",
    "        cols[4]:  _col(ws, \"H\", 10, 30),\n",
    "        cols[5]:  _col(ws, \"K\", 10, 30),\n",
    "        cols[6]:  _col(ws, \"L\", 10, 30),\n",
    "        cols[7]:  _col(ws, \"M\", 10, 30),\n",
    "        cols[8]:  _col(ws, \"N\", 10, 30),\n",
    "        cols[9]:  _col(ws, \"O\", 10, 30),\n",
    "        cols[10]: _col(ws, \"P\", 10, 30),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"FLENSBURG|HINWEIS|UMBENANNT\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be2f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_7(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.7”** – colour distribution by make.\n",
    "\n",
    "    Cleaning workflow\n",
    "    -----------------\n",
    "    1. Normalise header texts with `_clean_header()`.  \n",
    "       Duplicate titles get a numbered suffix via `_unique()`.\n",
    "    2. Build a DataFrame from column slices; drop fully-empty rows.\n",
    "    3. Run `_strip_cols()` again (idempotent safeguard).\n",
    "    4. Remove any row whose first column contains one of the keywords  \n",
    "       “INSGESAMT, DARUNTER, FLENSBURG, HINWEIS, UMBENANNT”.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Worksheet object pointing to “FZ 8.7”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Cleaned table ready to be concatenated with monthly data.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),  # Marke\n",
    "        _clean_header(ws[\"C8\"].value),  # Insgesamt\n",
    "        _clean_header(ws[\"D9\"].value),  # weiß\n",
    "        _clean_header(ws[\"E9\"].value),  # gelb\n",
    "        _clean_header(ws[\"F9\"].value),  # orange\n",
    "        _clean_header(ws[\"G9\"].value),  # rot\n",
    "        _clean_header(ws[\"H9\"].value),  # lila/violett\n",
    "        _clean_header(ws[\"I9\"].value),  # blau\n",
    "        _clean_header(ws[\"J9\"].value),  # grün\n",
    "        _clean_header(ws[\"K9\"].value),  # grau\n",
    "        _clean_header(ws[\"L9\"].value),  # braun\n",
    "        _clean_header(ws[\"M9\"].value),  # schwarz\n",
    "        _clean_header(ws[\"N9\"].value),  # sonstige\n",
    "    ]\n",
    "    cols = _unique(raw)                 # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 10, 100),\n",
    "        cols[1]:  _col(ws, \"C\", 10, 100),\n",
    "        cols[2]:  _col(ws, \"D\", 10, 100),\n",
    "        cols[3]:  _col(ws, \"E\", 10, 100),\n",
    "        cols[4]:  _col(ws, \"F\", 10, 100),\n",
    "        cols[5]:  _col(ws, \"G\", 10, 100),\n",
    "        cols[6]:  _col(ws, \"H\", 10, 100),\n",
    "        cols[7]:  _col(ws, \"I\", 10, 100),\n",
    "        cols[8]:  _col(ws, \"J\", 10, 100),\n",
    "        cols[9]:  _col(ws, \"K\", 10, 100),\n",
    "        cols[10]: _col(ws, \"L\", 10, 100),\n",
    "        cols[11]: _col(ws, \"M\", 10, 100),\n",
    "        cols[12]: _col(ws, \"N\", 10, 100),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|DARUNTER|FLENSBURG|HINWEIS|UMBENANNT\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14dfbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_8(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.8”** – vehicle distribution by engine-capacity class.\n",
    "\n",
    "    Cleaning workflow\n",
    "    -----------------\n",
    "    1. **Header normalisation** – `_clean_header()` removes new-lines and\n",
    "       duplicate spaces; `_unique()` adds numeric suffixes to duplicates.\n",
    "    2. **Drop empty rows** – `dropna(how=\"all\")`.\n",
    "    3. **Filter meta rows** – any row whose first column contains one of\n",
    "       the keywords  \n",
    "       “HINWEIS, HUBRAUM, FLENSBURG, UMBENANNT” is discarded.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Handle to sheet “FZ 8.8”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Clean table ready to be concatenated with other months.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),   # Lebensalter der Halterinnen und Halter\n",
    "        _clean_header(ws[\"C10\"].value),  # bis 1399\n",
    "        _clean_header(ws[\"D10\"].value),  # 1400 bis 1999\n",
    "        _clean_header(ws[\"E10\"].value),  # 2000 und mehr\n",
    "        _clean_header(ws[\"F10\"].value),  # unbekannt\n",
    "        _clean_header(ws[\"G9\"].value),   # Insgesamt\n",
    "        _clean_header(ws[\"H9\"].value),   # darunter Halterinnen\n",
    "    ]\n",
    "    cols = _unique(raw)                  # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]: _col(ws, \"B\", 11, 35),\n",
    "        cols[1]: _col(ws, \"C\", 11, 35),\n",
    "        cols[2]: _col(ws, \"D\", 11, 35),\n",
    "        cols[3]: _col(ws, \"E\", 11, 35),\n",
    "        cols[4]: _col(ws, \"F\", 11, 35),\n",
    "        cols[5]: _col(ws, \"G\", 11, 35),\n",
    "        cols[6]: _col(ws, \"H\", 11, 35),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"HINWEIS|HUBRAUM|FLENSBURG|UMBENANNT\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9e7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_9(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.9”** – distribution of holders (private vs. business).\n",
    "\n",
    "    Cleaning rules\n",
    "    --------------\n",
    "    1. Header texts are normalised with `_clean_header()`; duplicates\n",
    "       receive a numeric suffix via `_unique()`.\n",
    "    2. Entirely empty rows are dropped (`dropna(how=\"all\")`).\n",
    "    3. Rows whose first column matches one of the keywords\n",
    "       “INSGESAMT, HINWEIS, ERBRINGUNG, FLENSBURG, UMBENANNT”\n",
    "       are removed (totals / footnotes).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Worksheet object for the “FZ 8.9” tab.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Cleaned table ready for monthly concatenation.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),  # Marke\n",
    "        _clean_header(ws[\"C8\"].value),  # Insgesamt\n",
    "        _clean_header(ws[\"D8\"].value),  # Private Halterinnen und Halter\n",
    "        _clean_header(ws[\"F8\"].value),  # Gewerbliche Halterinnen und Halter\n",
    "        _clean_header(ws[\"H9\"].value),  # Kfz-Handel\n",
    "        _clean_header(ws[\"J9\"].value),  # Kfz-Herstellung\n",
    "        _clean_header(ws[\"L9\"].value),  # Kfz-Vermietung und Carsharing\n",
    "        _clean_header(ws[\"N9\"].value),  # Erbringung sonstiger Dienstleistungen \n",
    "        _clean_header(ws[\"P9\"].value),  # sonstige gewerbliche Halterinnen und Halter\n",
    "        _clean_header(ws[\"R8\"].value),  # Unbekannt\n",
    "    ]\n",
    "    cols = _unique(raw)                 # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 11, 100),\n",
    "        cols[1]:  _col(ws, \"C\", 11, 100),\n",
    "        cols[2]:  _col(ws, \"D\", 11, 100),\n",
    "        cols[3]:  _col(ws, \"F\", 11, 100),\n",
    "        cols[4]:  _col(ws, \"H\", 11, 100),\n",
    "        cols[5]:  _col(ws, \"J\", 11, 100),\n",
    "        cols[6]:  _col(ws, \"L\", 11, 100),\n",
    "        cols[7]:  _col(ws, \"N\", 11, 100),\n",
    "        cols[8]:  _col(ws, \"P\", 11, 100),\n",
    "        cols[9]:  _col(ws, \"R\", 11, 100),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|HINWEIS|ERBRINGUNG|FLENSBURG|UMBENANNT\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7231a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz8_16(ws):\n",
    "    \"\"\"\n",
    "    Parse sheet **“FZ 8.16”** (zulässige Gesamtmasse).\n",
    "\n",
    "    Cleaning rules\n",
    "    --------------\n",
    "    1. Header texts are normalised via `_clean_header()` to remove\n",
    "       new-lines and double spaces.  Duplicate names get a numeric\n",
    "       suffix from `_unique()`.\n",
    "    2. Entirely empty rows are dropped (`dropna(how=\"all\")`).\n",
    "    3. Rows whose first column contains any of the keywords  \n",
    "       “INSGESAMT, HINWEIS, SATTELANHÄNGER, VERORDNUNG, FLENSBURG”\n",
    "       are regarded as meta / totals and removed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "        Worksheet object pointing to sheet “FZ 8.16”.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Cleaned table ready for concatenation with other months.\n",
    "    \"\"\"\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),  # Zulässige Gesamtmasse in kg\n",
    "        _clean_header(ws[\"C8\"].value),  # Personenkraftwagen\n",
    "        _clean_header(ws[\"D9\"].value),  # darunter  Wohnmobile\n",
    "    ]\n",
    "    cols = _unique(raw)                 # avoid duplicate column names\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]: _col(ws, \"B\", 11, 50),\n",
    "        cols[1]: _col(ws, \"C\", 11, 50),\n",
    "        cols[2]: _col(ws, \"D\", 11, 50),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|HINWEIS|SATTELANHÄNGER|VERORDNUNG|FLENSBURG\"\n",
    "    mask = df[cols[0]].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    \n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[1:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047b9fa",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  VALIDATION FUNCTION  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79dc790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layouts of all FZ8 sheets are identical (coordinates, headers, first data row)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "LAYOUT-VALIDATION UTILITY FOR FZ-8 WORKBOOKS\n",
    "-------------------------------------------------------------------------\n",
    "Goal\n",
    "────\n",
    "Some Excel files (fz8_YYYYMM.xlsx) may have slightly shifted headers or\n",
    "data blocks.  This helper checks every workbook in *DATA_DIR* and tells\n",
    "you whether the coordinates and header texts are **exactly** the same\n",
    "for each sheet type (8.1 … 8.16).  Run it once before you parse data so\n",
    "you can fix misaligned sources early.\n",
    "\n",
    "How it works\n",
    "────────────\n",
    "1. `header_map`   – expected cell addresses that contain column titles.\n",
    "2. `data_start_row` – first row where real data (not headers) should start.\n",
    "3. For each sheet number N:\n",
    "      • read every workbook\n",
    "      • collect header texts from those coordinates\n",
    "      • compare to the first workbook (acts as “reference”)\n",
    "      • verify that *at least one* value exists in the first data row\n",
    "\n",
    "If anything is different, a human-readable list of issues is printed.\n",
    "Otherwise you get a green message that all layouts match.\n",
    "\n",
    "Extend / modify\n",
    "───────────────\n",
    "• Add / remove sheet numbers in `header_map`.\n",
    "• Update coordinates if the KBA changes its template.\n",
    "• Adjust `data_start_row` if header block grows/shrinks.\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "header_map = {\n",
    "    '1':  [\"B8\", \"C9\"],\n",
    "    '2':  [\"B8\", \"C8\", \"D8\", \"F9\", \"I9\", \"J9\", \"K10\"],\n",
    "    '3':  [\"B8\", \"C8\", \"D8\", \"E10\", \"F9\", \"G10\", \"H10\", \"I9\",\n",
    "           \"J10\", \"K10\", \"L9\", \"M9\", \"N9\", \"O10\", \"P9\"],\n",
    "    '6':  [\"B8\", \"C8\", \"D8\", \"G8\", \"H8\", \"K8\", \"L8\", \"M8\", \"N8\", \"O9\", \"P8\"],\n",
    "    '7':  [\"B8\", \"C8\", \"D9\", \"E9\", \"F9\", \"G9\", \"H9\", \"I9\", \"J9\",\n",
    "           \"K9\", \"L9\", \"M9\", \"N9\"],\n",
    "    '8':  [\"B8\", \"C10\", \"D10\", \"E10\", \"F10\", \"G9\", \"H9\"],\n",
    "    '9':  [\"B8\", \"C8\", \"D8\", \"F8\", \"H9\", \"J9\", \"L9\", \"N9\", \"P9\", \"R8\"],\n",
    "    '16': [\"B8\", \"C8\", \"D9\"],\n",
    "}\n",
    "data_start_row = {'1':10,'2':11,'3':11,'6':10,'7':10,'8':11,'9':11,'16':11}\n",
    "\n",
    "def check_fz8_layout():\n",
    "    \"\"\"\n",
    "    Validate that every “FZ 8.x” sheet in `DATA_DIR` is aligned exactly\n",
    "    like the first file encountered for that sheet number.\n",
    "\n",
    "    Prints a bullet list with every discrepancy; prints a success\n",
    "    message when no issues are found.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function is purely side-effecting (console output only).\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    for num, coords in header_map.items():\n",
    "        ref_names = None        # header texts from the first workbook\n",
    "        ref_file  = None        # its filename (for reference print)\n",
    "\n",
    "        for path in sorted(DATA_DIR.glob(\"fz8_*.xlsx\")):\n",
    "            wb  = load_workbook(path, data_only=True)\n",
    "            sn  = _find_sheet(wb, num)\n",
    "            if not sn:\n",
    "                issues.append(f\"{path.name}: workbook 8.{num} not found\")\n",
    "                continue\n",
    "            \n",
    "            # collect header texts at the expected coordinates\n",
    "            ws = wb[sn]\n",
    "            names = [_clean_header(ws[c].value) for c in coords]\n",
    "\n",
    "            # (1) compare to reference workbook\n",
    "            if ref_names is None:\n",
    "                ref_names, ref_file = names, path.name\n",
    "            elif names != ref_names:\n",
    "                issues.append(f\"{path.name}: 8.{num} – {names} ≠ {ref_names} (reference {ref_file})\")\n",
    "\n",
    "            # (2) make sure the first data row is populated\n",
    "            r0 = data_start_row[num]\n",
    "            if not any(ws[f\"{c[0]}{r0}\"].value for c in coords):\n",
    "                issues.append(f\"{path.name}: 8.{num} – row {r0} is empty, first data row shifted?\")\n",
    "    \n",
    "    # Report\n",
    "    if issues:\n",
    "        print(\"⚠️  Discrepancies have been detected:\")\n",
    "        for msg in issues:\n",
    "            print(\" •\", msg)\n",
    "    else:\n",
    "        print(\"The layouts of all FZ8 sheets are identical (coordinates, headers, first data row)\")\n",
    "\n",
    "# Run the check once:\n",
    "check_fz8_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accce272",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  DICT WORKBOOK → PARSER  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc973d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Sheet-ID → parser mapping\n",
    "-------------------------------------------------------------------------\n",
    "• Keys are the trailing digits of the sheet name “FZ 8.<N>”\n",
    "  kept as *strings*, because later we build filenames like:\n",
    "      f\"fz_8.{num}_raw.csv\"\n",
    "\n",
    "• Values are callables that take an `openpyxl.Worksheet` instance\n",
    "  and return a *cleaned* `pandas.DataFrame` for that sheet.\n",
    "\n",
    "Example\n",
    "-------\n",
    "    ws = workbook[\"FZ 8.3\"]\n",
    "    df = sheet_parsers[\"3\"](ws)\n",
    "\n",
    "`globals_by_sheet`\n",
    "------------------\n",
    "Pre-allocates one empty DataFrame per sheet ID.  \n",
    "During the main loop we simply `pd.concat()` every monthly chunk onto\n",
    "its accumulator, so when the loop ends each key contains **all rows**\n",
    "for that sheet across every workbook.\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "sheet_parsers = {\n",
    "    '2':  fz8_2, '3':  fz8_3, '6':  fz8_6,\n",
    "    '7':  fz8_7, '8':  fz8_8, '9':  fz8_9, '16': fz8_16,\n",
    "}\n",
    "\n",
    "# Accumulators: one global DataFrame per sheet number\n",
    "globals_by_sheet = {num: pd.DataFrame() for num in sheet_parsers}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ceb12",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  MAIN LOOP  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6aa3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Harvest every monthly workbook ─ append rows to the global accumulators\n",
    "-------------------------------------------------------------------------\n",
    "Workflow\n",
    "========\n",
    "1. Walk through all files that match “fz8_YYYYMM.xlsx”.\n",
    "   `sorted()` keeps them in chronological order (optional, but tidy).\n",
    "\n",
    "2. For each workbook:\n",
    "     • Load with `data_only=True` so formulas are resolved to values.\n",
    "     • Derive the period tag `YYYYMM` from the filename; this becomes\n",
    "       a new column **Date** in every parsed row.\n",
    "\n",
    "3. For each sheet ID in `sheet_parsers` (2, 3, 6, 7, 8, 9, 16):\n",
    "     • Locate the actual worksheet via `find_sheet()` — tolerant of\n",
    "       both “FZ 8.2” and “FZ8.2”.\n",
    "     • If the sheet is missing, print a warning and continue gracefully.\n",
    "     • Otherwise:\n",
    "         ▸ run its parser → clean `DataFrame`\n",
    "         ▸ prepend the **Date** column\n",
    "         ▸ `pd.concat()` onto the corresponding accumulator\n",
    "           in `globals_by_sheet`.\n",
    "\n",
    "Outcome\n",
    "-------\n",
    "After the loops finish, each entry in `globals_by_sheet`\n",
    "holds **all rows from every workbook** for that sheet type.\n",
    "They can be saved to disk or processed further in one go.\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "for path in sorted(DATA_DIR.glob(\"fz8_*.xlsx\")):\n",
    "    wb   = load_workbook(path, data_only=True)      # read Excel as values\n",
    "    date = _date_from_fname(path)                   # e.g. \"202401\"\n",
    "\n",
    "    for num, parser in sheet_parsers.items():\n",
    "        sname = _find_sheet(wb, num)                 # locate “FZ 8.<num>”\n",
    "        if not sname:                               # skip missing sheets\n",
    "            print(f\"{path.name}: workbook 8.{num} not found\")\n",
    "            continue\n",
    "\n",
    "        df = parser(wb[sname])                      # parse & clean\n",
    "        df.insert(0, \"DATE\", date)                  # add period column\n",
    "\n",
    "        # append to the global accumulator for this sheet\n",
    "        globals_by_sheet[num] = pd.concat([globals_by_sheet[num], df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ad2df",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  SAVE RAW CSVs  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bce990a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Saved fz_8.2_2023-2025_raw.csv  →  (1653, 8)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1653 entries, 0 to 1652\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   DATE                  1653 non-null   object\n",
      " 1   MARKE                 1653 non-null   object\n",
      " 2   ANZAHL                1653 non-null   object\n",
      " 3   CO2-EMISSION IN G/KM  1653 non-null   object\n",
      " 4   EURO 6                1653 non-null   object\n",
      " 5   ELEKTRO (BEV)         1653 non-null   object\n",
      " 6   HYBRID                1653 non-null   object\n",
      " 7   DARUNTER PLUG-IN      1653 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 103.4+ KB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_8.3_2023-2025_raw.csv  →  (10146, 16)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10146 entries, 0 to 10145\n",
      "Data columns (total 16 columns):\n",
      " #   Column                                Non-Null Count  Dtype \n",
      "---  ------                                --------------  ----- \n",
      " 0   DATE                                  10146 non-null  object\n",
      " 1   SEGMENT                               10146 non-null  object\n",
      " 2   MODELLREIHE                           10146 non-null  object\n",
      " 3   INSGESAMT                             10146 non-null  object\n",
      " 4   CO2-EMISSION IN G/KM                  10146 non-null  object\n",
      " 5   BENZIN                                10146 non-null  object\n",
      " 6   CO2-EMISSION IN G/KM1                 10146 non-null  object\n",
      " 7   KRAFTSTOFFVERBRAUCH IN L/100 KM       10146 non-null  object\n",
      " 8   DIESEL                                10146 non-null  object\n",
      " 9   CO2-EMISSION IN G/KM2                 10146 non-null  object\n",
      " 10  KRAFTSTOFFVERBRAUCH IN L/100 KM1      10146 non-null  object\n",
      " 11  ERDGAS (CNG) (EINSCHL. BIVALENT)      10146 non-null  object\n",
      " 12  FLUSSIGGAS (LPG) (EINSCHL. BIVALENT)  10146 non-null  object\n",
      " 13  HYBRID                                10146 non-null  object\n",
      " 14  DAR. PLUG-IN                          10146 non-null  object\n",
      " 15  ELEKTRO (BEV)                         10146 non-null  object\n",
      "dtypes: object(16)\n",
      "memory usage: 1.2+ MB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_8.6_2023-2025_raw.csv  →  (504, 12)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 504 entries, 0 to 503\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                Non-Null Count  Dtype \n",
      "---  ------                                --------------  ----- \n",
      " 0   DATE                                  504 non-null    object\n",
      " 1   BUNDESLAND                            504 non-null    object\n",
      " 2   BENZIN INSGESAMT                      504 non-null    object\n",
      " 3   DARUNTER EURO 6                       504 non-null    object\n",
      " 4   DIESEL INSGESAMT                      504 non-null    object\n",
      " 5   DARUNTER EURO 61                      504 non-null    object\n",
      " 6   FLUSSIGGAS (LPG) (EINSCHL. BIVALENT)  504 non-null    object\n",
      " 7   ERDGAS (CNG) (EINSCHL. BIVALENT)      504 non-null    object\n",
      " 8   ELEKTRO (BEV)                         504 non-null    object\n",
      " 9   HYBRID                                504 non-null    object\n",
      " 10  DARUNTER PLUG-IN                      504 non-null    object\n",
      " 11  SONSTIGE                              504 non-null    object\n",
      "dtypes: object(12)\n",
      "memory usage: 47.4+ KB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_8.7_2023-2025_raw.csv  →  (1657, 14)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1657 entries, 0 to 1656\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   DATE          1657 non-null   object\n",
      " 1   MARKE         1657 non-null   object\n",
      " 2   INSGESAMT     1657 non-null   object\n",
      " 3   WEISS         1657 non-null   object\n",
      " 4   GELB          1657 non-null   object\n",
      " 5   ORANGE        1657 non-null   object\n",
      " 6   ROT           1657 non-null   object\n",
      " 7   LILA/VIOLETT  1657 non-null   object\n",
      " 8   BLAU          1657 non-null   object\n",
      " 9   GRUN          1657 non-null   object\n",
      " 10  GRAU          1657 non-null   object\n",
      " 11  BRAUN         1657 non-null   object\n",
      " 12  SCHWARZ       1657 non-null   object\n",
      " 13  SONSTIGE      1657 non-null   object\n",
      "dtypes: object(14)\n",
      "memory usage: 181.4+ KB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_8.8_2023-2025_raw.csv  →  (588, 8)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 588 entries, 0 to 587\n",
      "Data columns (total 8 columns):\n",
      " #   Column                                  Non-Null Count  Dtype \n",
      "---  ------                                  --------------  ----- \n",
      " 0   DATE                                    588 non-null    object\n",
      " 1   LEBENSALTER DER HALTERINNEN UND HALTER  588 non-null    object\n",
      " 2   BIS 1399                                588 non-null    object\n",
      " 3   1400 BIS 1999                           588 non-null    object\n",
      " 4   2000 UND MEHR                           588 non-null    object\n",
      " 5   UNBEKANNT                               588 non-null    object\n",
      " 6   INSGESAMT                               588 non-null    object\n",
      " 7   DARUNTER HALTERINNEN                    588 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 36.9+ KB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_8.9_2023-2025_raw.csv  →  (1653, 11)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1653 entries, 0 to 1652\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                       Non-Null Count  Dtype \n",
      "---  ------                                       --------------  ----- \n",
      " 0   DATE                                         1653 non-null   object\n",
      " 1   MARKE                                        1653 non-null   object\n",
      " 2   INSGESAMT                                    1653 non-null   object\n",
      " 3   PRIVATE HALTERINNEN UND HALTER               1653 non-null   object\n",
      " 4   GEWERBLICHE HALTERINNEN UND HALTER           1653 non-null   object\n",
      " 5   KFZ-HANDEL                                   1653 non-null   object\n",
      " 6   KFZ-HERSTELLUNG                              1653 non-null   object\n",
      " 7   KFZ-VERMIETUNG UND CARSHARING                1653 non-null   object\n",
      " 8   ERBRINGUNG SONSTIGER DIENSTLEISTUNGEN        1653 non-null   object\n",
      " 9   SONSTIGE GEWERBLICHE HALTERINNEN UND HALTER  1653 non-null   object\n",
      " 10  UNBEKANNT                                    1653 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 142.2+ KB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_8.16_2023-2025_raw.csv  →  (924, 4)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 924 entries, 0 to 923\n",
      "Data columns (total 4 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   DATE                         924 non-null    object\n",
      " 1   ZULASSIGE GESAMTMASSE IN KG  924 non-null    object\n",
      " 2   PERSONENKRAFTWAGEN           924 non-null    object\n",
      " 3   DARUNTER WOHNMOBILE          924 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 29.0+ KB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Export 2023-2025 “raw” CSVs — one per sheet\n",
    "-------------------------------------------------------------------------\n",
    "• Each accumulator in `globals_by_sheet` now contains ONLY rows parsed\n",
    "  from the 2023-2025 Excel workbooks.\n",
    "• We store them as text-only CSV files named\n",
    "        fz_8.<N>_2023-2025_raw.csv\n",
    "  inside `OUT_DIR` (../data/raw/fz8/csv).\n",
    "• Prior to saving we\n",
    "      1. Convert every NaN → \"\"     (`fillna('')`)\n",
    "      2. Cast every column to str   (`astype(str)`)\n",
    "  so downstream tools will not see mixed dtypes.\n",
    "• A compact log message is printed for each file, followed by\n",
    "  `df.info()` (memory footprint + dtype overview) for manual sanity\n",
    "  checks.\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "for num, df in globals_by_sheet.items():\n",
    "    # ensure 100 % string representation, no NaN\n",
    "    df = df.fillna('').astype(str)\n",
    "\n",
    "    # path …/csv/fz_8.<num>_2023-2025_raw.csv\n",
    "    out_csv = OUT_DIR / f\"fz_8.{num}_2023-2025_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # console log\n",
    "    print(f\"• Saved {out_csv.name}  →  {df.shape}\\n\")\n",
    "    df.info()           # quick dtype audit\n",
    "    print(\"\\n\\n\")       # visual separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97968e",
   "metadata": {},
   "source": [
    "## ──────────────────────────  CONCAT WITH 2020-2022  ──────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0960813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fz_8.2_raw.csv  →  (2949, 8)\n",
      "fz_8.3_raw.csv  →  (21820, 16)\n",
      "fz_8.6_raw.csv  →  (936, 12)\n",
      "fz_8.7_raw.csv  →  (2953, 14)\n",
      "fz_8.8_raw.csv  →  (1344, 8)\n",
      "fz_8.9_raw.csv  →  (2949, 11)\n",
      "fz_8.16_raw.csv  →  (2112, 4)\n",
      "\n",
      "All sheets have been successfully validated and merged.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-------------------------------------------------------------------------\n",
    "Merge “old” (2020-2022) and “new” (2023-2025) raw CSVs\n",
    "-------------------------------------------------------------------------\n",
    "Why two eras?\n",
    "  • 2020-2022 data come from a single “PDF layout” workbook (§1 script).\n",
    "  • 2023-2025 data are parsed month-by-month (§2 script).\n",
    "\n",
    "Guard-rails\n",
    "  1. We merge only if **both** CSV files exist.\n",
    "  2. Headers must match *exactly* (same count, order, spelling).\n",
    "     Otherwise we log a mismatch and skip that sheet.\n",
    "\n",
    "Result\n",
    "  One unified CSV per sheet:\n",
    "      ../data/processed/fz_8.<N>_raw.csv\n",
    "  All cells stored as text; NaN → \"\".\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "sheet_nums = [\"2\", \"3\", \"6\", \"7\", \"8\", \"9\", \"16\"]\n",
    "issues = []\n",
    "\n",
    "for num in sheet_nums:\n",
    "    f_old = OUT_DIR / f\"fz_8.{num}_2020-2022_raw.csv\"\n",
    "    f_new = OUT_DIR / f\"fz_8.{num}_2023-2025_raw.csv\"\n",
    "\n",
    "    # existence check\n",
    "    if not f_old.exists() or not f_new.exists():\n",
    "        issues.append(f\"8.{num}: missing {'old' if not f_old.exists() else 'new'} file\")\n",
    "        continue\n",
    "\n",
    "    df_old = pd.read_csv(f_old, dtype=str)\n",
    "    df_new = pd.read_csv(f_new, dtype=str)\n",
    "\n",
    "    # header consistency check\n",
    "    if list(df_old.columns) != list(df_new.columns):\n",
    "        issues.append(\n",
    "            f\"8.{num}: header mismatch.\\n\"\n",
    "            f\"  old: {list(df_old.columns)}\\n\"\n",
    "            f\"  new: {list(df_new.columns)}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # concatenate & export\n",
    "    df_all = (\n",
    "        pd.concat([df_old, df_new], ignore_index=True)\n",
    "          .fillna(\"\")       # NaN → empty string\n",
    "          .astype(str)      # guarantee text dtype\n",
    "    )\n",
    "\n",
    "    out_csv = DST_DIR / f\"fz_08.{num}_raw.csv\"\n",
    "    df_all.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"fz_8.{num}_raw.csv  →  {df_all.shape}\")\n",
    "\n",
    "# summary report\n",
    "if issues:\n",
    "    print(\"\\nDiscrepancies detected:\")\n",
    "    for msg in issues:\n",
    "        print(\" •\", msg)\n",
    "else:\n",
    "    print(\"\\nAll sheets have been successfully validated and merged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
