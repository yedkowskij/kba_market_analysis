{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e9699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DATA_DIR = Path(\"../data/raw/fz2\")          # input *.xlsx files\n",
    "OUT_DIR  = Path(\"../data/raw/fz2/csv\")      # will hold *_raw.csv\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DST_DIR = Path(\"../data/processed/,\")         # final merged CSVs\n",
    "DST_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83330ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _date_from_fname(p):\n",
    "    \"\"\"Return YYYYMM extracted from filename `fz2_YYYY.xlsx`.\"\"\"\n",
    "    return re.search(r\"(\\d{4})\", p.name).group(1)\n",
    "\n",
    "def _col(ws, letter, r0, r1):\n",
    "    \"\"\"Return values of column *letter* from rows *r0 … r1* inclusive.\"\"\"\n",
    "    return [ws[f\"{letter}{row}\"].value for row in range(r0, r1 + 1)]\n",
    "\n",
    "# def _clean_header(s):\n",
    "#     \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "#     return str(s).replace('\\n', ' ').replace('  ', ' ').strip() if s is not None else s\n",
    "\n",
    "def _clean_header(s):\n",
    "    \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "    return (str(s).translate(str.maketrans(\"äÄöÖüÜ\", \"aAoOuU\")).replace(\"\\n\", \" \").replace(\"  \", \" \").strip().upper()) if s is not None else s\n",
    "\n",
    "def _strip_cols(df):\n",
    "    \"\"\"Apply `clean_header` to every column name in-place and return df.\"\"\"\n",
    "    df.columns = [_clean_header(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique(cols):\n",
    "    \"\"\"Ensure uniqueness by adding numeric suffixes.\"\"\"\n",
    "    seen, out = {}, []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}{seen[c]}\")\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def _find_sheet(wb, num):\n",
    "    \"\"\"Locate sheet whose name matches *pattern* (case-insensitive).\"\"\"\n",
    "    pattern = re.compile(fr\"^FZ\\s*2\\.{re.escape(num)}$\", flags=re.IGNORECASE)\n",
    "    for name in wb.sheetnames:\n",
    "        if pattern.match(name.strip()):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# def _strip_upper(df):\n",
    "#     \"\"\"\n",
    "#     Trim whitespace and up-case every object column (in-place).\n",
    "#     Faster & future-proof replacement for the old `applymap`.\n",
    "#     \"\"\"\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype == \"object\":\n",
    "#             df[col] = df[col].str.strip().str.upper()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc95fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz2_2(ws):\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),   # \n",
    "        _clean_header(ws[\"C8\"].value),   # \n",
    "        _clean_header(ws[\"D8\"].value),  # \n",
    "        _clean_header(ws[\"E8\"].value),   # \n",
    "        _clean_header(ws[\"F8\"].value),  # \n",
    "        _clean_header(ws[\"G8\"].value),  # \n",
    "        _clean_header(ws[\"H8\"].value),   # \n",
    "        _clean_header(ws[\"I8\"].value),  # \n",
    "        _clean_header(ws[\"J9\"].value),  # \n",
    "        _clean_header(ws[\"K9\"].value),   # \n",
    "        _clean_header(ws[\"L9\"].value),   # \n",
    "        _clean_header(ws[\"M9\"].value),   # \n",
    "        _clean_header(ws[\"N9\"].value),  # \n",
    "    ]\n",
    "    cols = _unique(raw)                  # avoid duplicate column names\n",
    "\n",
    "\n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 10, 20000),\n",
    "        cols[1]:  _col(ws, \"C\", 10, 20000),\n",
    "        cols[2]:  _col(ws, \"D\", 10, 20000),\n",
    "        cols[3]:  _col(ws, \"E\", 10, 20000),\n",
    "        cols[4]:  _col(ws, \"F\", 10, 20000),\n",
    "        cols[5]:  _col(ws, \"G\", 10, 20000),\n",
    "        cols[6]:  _col(ws, \"H\", 10, 20000),\n",
    "        cols[7]:  _col(ws, \"I\", 10, 20000),\n",
    "        cols[8]:  _col(ws, \"J\", 10, 20000),\n",
    "        cols[9]:  _col(ws, \"K\", 10, 20000),\n",
    "        cols[10]: _col(ws, \"L\", 10, 20000),\n",
    "        cols[11]: _col(ws, \"M\", 10, 20000),\n",
    "        cols[12]: _col(ws, \"N\", 10, 20000),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "    \n",
    "    # find the actual column name\n",
    "    seg_col = next(c for c in df.columns if \"HERSTELLER\" in c)\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"ZUSAMMEN|INSGESAMT|HINWEIS|FLENSBURG|SONSTIGE\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    # find the actual column name\n",
    "    seg_col = next(c for c in df.columns if \"HANDELSNAME\" in c)\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    df[seg_col] = (df[seg_col].astype(str).str.replace(\",\", \";\", regex=False))\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"SONSTIGE\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[3:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb34b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz2_4(ws):\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),   # \n",
    "        _clean_header(ws[\"C8\"].value),   # \n",
    "        _clean_header(ws[\"D8\"].value),  # \n",
    "        _clean_header(ws[\"E8\"].value),   # \n",
    "        _clean_header(ws[\"F8\"].value),  # \n",
    "        _clean_header(ws[\"G8\"].value),  # \n",
    "        _clean_header(ws[\"H8\"].value),   # \n",
    "        _clean_header(ws[\"I8\"].value),  # \n",
    "        _clean_header(ws[\"J8\"].value),  # \n",
    "        _clean_header(ws[\"K8\"].value),   # \n",
    "        _clean_header(ws[\"L8\"].value),   # \n",
    "        _clean_header(ws[\"M8\"].value),   # \n",
    "        _clean_header(ws[\"N8\"].value),  # \n",
    "        _clean_header(ws[\"O8\"].value),  # \n",
    "        _clean_header(ws[\"P8\"].value),  # \n",
    "        _clean_header(ws[\"Q8\"].value),   # \n",
    "        _clean_header(ws[\"R8\"].value),  # \n",
    "        _clean_header(ws[\"S8\"].value),  # \n",
    "        _clean_header(ws[\"T8\"].value),   # \n",
    "        _clean_header(ws[\"U8\"].value),   # \n",
    "        _clean_header(ws[\"V8\"].value),   # \n",
    "    ]\n",
    "    cols = _unique(raw)                  # avoid duplicate column names\n",
    "\n",
    "\n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 9, 20000),\n",
    "        cols[1]:  _col(ws, \"C\", 9, 20000),\n",
    "        cols[2]:  _col(ws, \"D\", 9, 20000),\n",
    "        cols[3]:  _col(ws, \"E\", 9, 20000),\n",
    "        cols[4]:  _col(ws, \"F\", 9, 20000),\n",
    "        cols[5]:  _col(ws, \"G\", 9, 20000),\n",
    "        cols[6]:  _col(ws, \"H\", 9, 20000),\n",
    "        cols[7]:  _col(ws, \"I\", 9, 20000),\n",
    "        cols[8]:  _col(ws, \"J\", 9, 20000),\n",
    "        cols[9]:  _col(ws, \"K\", 9, 20000),\n",
    "        cols[10]: _col(ws, \"L\", 9, 20000),\n",
    "        cols[11]: _col(ws, \"M\", 9, 20000),\n",
    "        cols[12]: _col(ws, \"N\", 9, 20000),\n",
    "        cols[13]: _col(ws, \"O\", 9, 20000),\n",
    "        cols[14]: _col(ws, \"P\", 9, 20000),\n",
    "        cols[15]: _col(ws, \"Q\", 9, 20000),\n",
    "        cols[16]: _col(ws, \"R\", 9, 20000),\n",
    "        cols[17]: _col(ws, \"S\", 9, 20000),\n",
    "        cols[18]: _col(ws, \"T\", 9, 20000),\n",
    "        cols[19]: _col(ws, \"U\", 9, 20000),\n",
    "        cols[20]: _col(ws, \"V\", 9, 20000),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "    \n",
    "    # find the actual column name\n",
    "    seg_col = next(c for c in df.columns if \"HERSTELLER\" in c)\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"ZUSAMMEN|INSGESAMT|HINWEIS|FLENSBURG|SONSTIGE\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    # find the actual column name\n",
    "    seg_col = next(c for c in df.columns if \"HANDELSNAME\" in c)\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    df[seg_col] = (df[seg_col].astype(str).str.replace(\",\", \";\", regex=False))\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"SONSTIGE\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[2:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c83f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layouts of all FZ2 sheets are identical (coordinates, headers, first data row)\n"
     ]
    }
   ],
   "source": [
    "header_map = {\n",
    "    '2':  [\"B8\", \"C8\", \"D8\", \"E8\", \"F8\", \"G8\", \"H8\", \"I8\", \"J9\", \"K9\", \"L9\", \"M9\", \"N9\"],\n",
    "    '4':  [\"B8\", \"C8\", \"D8\", \"E8\", \"F8\", \"G8\", \"H8\", \"I8\", \"J8\", \"K8\", \"L8\", \"M8\", \"N8\", \"O8\", \"P8\", \"Q8\", \"R8\", \"S8\", \"T8\", \"U8\", \"V8\"],\n",
    "}\n",
    "\n",
    "data_start_row = {'2':10, '4':9}\n",
    "\n",
    "def check_fz2_layout():\n",
    "    issues = []\n",
    "    for num, coords in header_map.items():\n",
    "        ref_names = None        # header texts from the first workbook\n",
    "        ref_file  = None        # its filename (for reference print)\n",
    "\n",
    "        for path in sorted(DATA_DIR.glob(\"fz2_*.xlsx\")):\n",
    "            wb  = load_workbook(path, data_only=True)\n",
    "            sn  = _find_sheet(wb, num)\n",
    "            if not sn:\n",
    "                issues.append(f\"{path.name}: workbook 2.{num} not found\")\n",
    "                continue\n",
    "            \n",
    "            # collect header texts at the expected coordinates\n",
    "            ws = wb[sn]\n",
    "            names = [_clean_header(ws[c].value) for c in coords]\n",
    "\n",
    "            # (1) compare to reference workbook\n",
    "            if ref_names is None:\n",
    "                ref_names, ref_file = names, path.name\n",
    "            elif names != ref_names:\n",
    "                issues.append(f\"{path.name}: 2.{num} – {names} ≠ {ref_names} (reference {ref_file})\")\n",
    "\n",
    "            # (2) make sure the first data row is populated\n",
    "            r0 = data_start_row[num]\n",
    "            if not any(ws[f\"{c[0]}{r0}\"].value for c in coords):\n",
    "                issues.append(f\"{path.name}: 2.{num} – row {r0} is empty, first data row shifted?\")\n",
    "    \n",
    "    # Report\n",
    "    if issues:\n",
    "        print(\"⚠️  Discrepancies have been detected:\")\n",
    "        for msg in issues:\n",
    "            print(\" •\", msg)\n",
    "    else:\n",
    "        print(\"The layouts of all FZ2 sheets are identical (coordinates, headers, first data row)\")\n",
    "\n",
    "# Run the check once:\n",
    "check_fz2_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e85f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_parsers = {'2':  fz2_2, '4':  fz2_4,}\n",
    "\n",
    "# Accumulators: one global DataFrame per sheet number\n",
    "globals_by_sheet = {num: pd.DataFrame() for num in sheet_parsers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd1f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sorted(DATA_DIR.glob(\"fz2_*.xlsx\")):\n",
    "    wb   = load_workbook(path, data_only=True)      # read Excel as values\n",
    "    date = _date_from_fname(path)                   # e.g. \"2024\"\n",
    "\n",
    "    for num, parser in sheet_parsers.items():\n",
    "        sname = _find_sheet(wb, num)                 # locate “FZ 2.<num>”\n",
    "        if not sname:                               # skip missing sheets\n",
    "            print(f\"{path.name}: workbook 2.{num} not found\")\n",
    "            continue\n",
    "\n",
    "        df = parser(wb[sname])                      # parse & clean\n",
    "        df.insert(0, \"Date\", date)                  # add period column\n",
    "\n",
    "        # append to the global accumulator for this sheet\n",
    "        globals_by_sheet[num] = pd.concat([globals_by_sheet[num], df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1895e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Saved fz_2.2_raw.csv  →  (77912, 14)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77912 entries, 0 to 77911\n",
      "Data columns (total 14 columns):\n",
      " #   Column                               Non-Null Count  Dtype \n",
      "---  ------                               --------------  ----- \n",
      " 0   Date                                 77912 non-null  object\n",
      " 1   HERSTELLER                           77912 non-null  object\n",
      " 2   HANDELSNAME                          77912 non-null  object\n",
      " 3   TYP-SCHL.-NR.                        77912 non-null  object\n",
      " 4   KW                                   77912 non-null  object\n",
      " 5   KRAFTSTOFFART                        77912 non-null  object\n",
      " 6   ALLRAD                               77912 non-null  object\n",
      " 7   AUFBAUART                            77912 non-null  object\n",
      " 8   INSGESAMT                            77912 non-null  object\n",
      " 9   WOHNMOBILE                           77912 non-null  object\n",
      " 10  PRIVATE HALTERINNEN UND HALTER       77912 non-null  object\n",
      " 11  HALTERINNEN UND HALTER BIS 29 JAHRE  77912 non-null  object\n",
      " 12  HALTERINNEN UND HALTER AB 60 JAHRE   77912 non-null  object\n",
      " 13  HALTERINNEN                          77912 non-null  object\n",
      "dtypes: object(14)\n",
      "memory usage: 8.3+ MB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_2.4_raw.csv  →  (77869, 22)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77869 entries, 0 to 77868\n",
      "Data columns (total 22 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Date                     77869 non-null  object\n",
      " 1   HERSTELLER               77869 non-null  object\n",
      " 2   HANDELSNAME              77869 non-null  object\n",
      " 3   TYP-SCHL.-NR.            77869 non-null  object\n",
      " 4   BADEN- WURTTEMBERG       77869 non-null  object\n",
      " 5   BAYERN                   77869 non-null  object\n",
      " 6   BERLIN                   77869 non-null  object\n",
      " 7   BRANDENBURG              77869 non-null  object\n",
      " 8   BREMEN                   77869 non-null  object\n",
      " 9   HAMBURG                  77869 non-null  object\n",
      " 10  HESSEN                   77869 non-null  object\n",
      " 11  MECKLENBURG- VORPOMMERN  77869 non-null  object\n",
      " 12  NIEDER- SACHSEN          77869 non-null  object\n",
      " 13  NORDRHEIN- WESTFALEN     77869 non-null  object\n",
      " 14  RHEINLAND- PFALZ         77869 non-null  object\n",
      " 15  SAARLAND                 77869 non-null  object\n",
      " 16  SACHSEN                  77869 non-null  object\n",
      " 17  SACHSEN- ANHALT          77869 non-null  object\n",
      " 18  SCHLESWIG- HOLSTEIN      77869 non-null  object\n",
      " 19  THURINGEN                77869 non-null  object\n",
      " 20  SONSTIGE                 77869 non-null  object\n",
      " 21  DEUTSCHLAND              77869 non-null  object\n",
      "dtypes: object(22)\n",
      "memory usage: 13.1+ MB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, df in globals_by_sheet.items():\n",
    "    # ensure 100 % string representation, no NaN\n",
    "    df = df.fillna('').astype(str)\n",
    "\n",
    "    # path …/csv/fz_2.<num>_raw.csv\n",
    "    out_csv = OUT_DIR / f\"fz_2.{num}_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    out_csv = DST_DIR / f\"fz_2.{num}_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # console log\n",
    "    print(f\"• Saved {out_csv.name}  →  {df.shape}\\n\")\n",
    "    df.info()           # quick dtype audit\n",
    "    print(\"\\n\\n\")       # visual separator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
