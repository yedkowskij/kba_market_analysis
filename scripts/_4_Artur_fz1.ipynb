{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f65858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DATA_DIR = Path(\"../data/raw/fz1\")          # input *.xlsx files\n",
    "OUT_DIR  = Path(\"../data/raw/fz1/csv\")      # will hold *_raw.csv\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DST_DIR = Path(\"../data/processed/,\")         # final merged CSVs\n",
    "DST_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006692ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _date_from_fname(p):\n",
    "    \"\"\"Return YYYYMM extracted from filename `fz1_YYYY.xlsx`.\"\"\"\n",
    "    return re.search(r\"(\\d{4})\", p.name).group(1)\n",
    "\n",
    "def _col(ws, letter, r0, r1):\n",
    "    \"\"\"Return values of column *letter* from rows *r0 … r1* inclusive.\"\"\"\n",
    "    return [ws[f\"{letter}{row}\"].value for row in range(r0, r1 + 1)]\n",
    "\n",
    "# def _clean_header(s):\n",
    "#     \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "#     return str(s).replace('\\n', ' ').replace('  ', ' ').strip() if s is not None else s\n",
    "\n",
    "def _clean_header(s):\n",
    "    \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "    return (str(s).translate(str.maketrans(\"äÄöÖüÜ\", \"aAoOuU\")).replace(\"\\n\", \" \").replace(\"  \", \" \").strip().upper()) if s is not None else s\n",
    "\n",
    "def _strip_cols(df):\n",
    "    \"\"\"Apply `clean_header` to every column name in-place and return df.\"\"\"\n",
    "    df.columns = [_clean_header(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique(cols):\n",
    "    \"\"\"Ensure uniqueness by adding numeric suffixes.\"\"\"\n",
    "    seen, out = {}, []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}{seen[c]}\")\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def _find_sheet(wb, num):\n",
    "    \"\"\"Locate sheet whose name matches *pattern* (case-insensitive).\"\"\"\n",
    "    pattern = re.compile(fr\"^FZ\\s*1\\.{re.escape(num)}$\", flags=re.IGNORECASE)\n",
    "    for name in wb.sheetnames:\n",
    "        if pattern.match(name.strip()):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "# def _strip_upper(df):\n",
    "#     \"\"\"\n",
    "#     Trim whitespace and up-case every object column (in-place).\n",
    "#     Faster & future-proof replacement for the old `applymap`.\n",
    "#     \"\"\"\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype == \"object\":\n",
    "#             df[col] = df[col].str.strip().str.upper()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b880bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz1_1(ws):\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B9\"].value),   # \n",
    "        _clean_header(ws[\"D9\"].value),   # \n",
    "        _clean_header(ws[\"J8\"].value),  # \n",
    "        _clean_header(ws[\"K9\"].value),   # \n",
    "        _clean_header(ws[\"L9\"].value),  # \n",
    "        _clean_header(ws[\"M9\"].value),  # \n",
    "        _clean_header(ws[\"N9\"].value),   # \n",
    "        _clean_header(ws[\"O9\"].value),  # \n",
    "        _clean_header(ws[\"P9\"].value),  # \n",
    "        _clean_header(ws[\"Q9\"].value),   # \n",
    "        _clean_header(ws[\"R9\"].value),   # \n",
    "        _clean_header(ws[\"S9\"].value),   # \n",
    "        _clean_header(ws[\"T9\"].value),  # \n",
    "        _clean_header(ws[\"U9\"].value),   # \n",
    "    ]\n",
    "    cols = _unique(raw)                  # avoid duplicate column names\n",
    "\n",
    "\n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 10, 500),\n",
    "        cols[1]:  _col(ws, \"D\", 10, 500),\n",
    "        cols[2]:  _col(ws, \"J\", 10, 500),\n",
    "        cols[3]:  _col(ws, \"K\", 10, 500),\n",
    "        cols[4]:  _col(ws, \"L\", 10, 500),\n",
    "        cols[5]:  _col(ws, \"M\", 10, 500),\n",
    "        cols[6]:  _col(ws, \"N\", 10, 500),\n",
    "        cols[7]:  _col(ws, \"O\", 10, 500),\n",
    "        cols[8]:  _col(ws, \"P\", 10, 500),\n",
    "        cols[9]:  _col(ws, \"Q\", 10, 500),\n",
    "        cols[10]: _col(ws, \"R\", 10, 500),\n",
    "        cols[11]: _col(ws, \"S\", 10, 500),\n",
    "        cols[12]: _col(ws, \"T\", 10, 500),\n",
    "        cols[13]: _col(ws, \"U\", 10, 500),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "    \n",
    "    # find the actual column name for Land\n",
    "    seg_col = next(c for c in df.columns if \"LAND\" in c)\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|HINWEIS|FLENSBURG|REVIDIERT|SONSTIGE\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    df[seg_col] = (df[seg_col].astype(str).str.replace(\"UE\", \"U\", regex=False))\n",
    "\n",
    "    seg_col = next(c for c in df.columns if \"STATISTISCHE\" in c)\n",
    "    df = df.dropna(subset=[seg_col])\n",
    "    df = df[df[seg_col].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[2:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16fc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz1_2(ws):\n",
    "    raw = [\n",
    "        _clean_header(ws[\"B8\"].value),   # \n",
    "        _clean_header(ws[\"D8\"].value),   # \n",
    "        _clean_header(ws[\"E8\"].value),  # \n",
    "        _clean_header(ws[\"F9\"].value),  # \n",
    "        _clean_header(ws[\"G9\"].value),  # \n",
    "        _clean_header(ws[\"H9\"].value),   # \n",
    "        _clean_header(ws[\"I9\"].value),  # \n",
    "        _clean_header(ws[\"K9\"].value),  # \n",
    "        _clean_header(ws[\"L9\"].value),   # \n",
    "        _clean_header(ws[\"M9\"].value),   # \n",
    "        _clean_header(ws[\"N9\"].value),   # \n",
    "        _clean_header(ws[\"O9\"].value),  # \n",
    "        _clean_header(ws[\"P9\"].value),   # \n",
    "        _clean_header(ws[\"Q9\"].value),   # \n",
    "        _clean_header(ws[\"R9\"].value),   # \n",
    "        _clean_header(ws[\"V9\"].value),   # \n",
    "    ]\n",
    "    cols = _unique(raw)                  # avoid duplicate column names\n",
    "\n",
    "\n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        cols[0]:  _col(ws, \"B\", 10, 500),\n",
    "        cols[1]:  _col(ws, \"D\", 10, 500),\n",
    "        cols[2]:  _col(ws, \"E\", 10, 500),\n",
    "        cols[3]:  _col(ws, \"F\", 10, 500),\n",
    "        cols[4]:  _col(ws, \"G\", 10, 500),\n",
    "        cols[5]:  _col(ws, \"H\", 10, 500),\n",
    "        cols[6]:  _col(ws, \"I\", 10, 500),\n",
    "        cols[7]:  _col(ws, \"K\", 10, 500),\n",
    "        cols[8]:  _col(ws, \"L\", 10, 500),\n",
    "        cols[9]: _col(ws, \"M\", 10, 500),\n",
    "        cols[10]: _col(ws, \"N\", 10, 500),\n",
    "        cols[11]: _col(ws, \"O\", 10, 500),\n",
    "        cols[12]: _col(ws, \"P\", 10, 500),\n",
    "        cols[13]: _col(ws, \"Q\", 10, 500),\n",
    "        cols[14]: _col(ws, \"R\", 10, 500),\n",
    "        cols[15]: _col(ws, \"V\", 10, 500),\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "    \n",
    "    # find the actual column name for Land\n",
    "    seg_col = next(c for c in df.columns if \"LAND\" in c)\n",
    "    df[seg_col] = df[seg_col].ffill()\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|HINWEIS|FLENSBURG|REVIDIERT|SONSTIGE\"\n",
    "    mask = df[seg_col].astype(str).str.contains(trash, case=False, na=False)\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "\n",
    "    df[seg_col] = (df[seg_col].astype(str).str.replace(\"UE\", \"U\", regex=False))\n",
    "\n",
    "    seg_col = next(c for c in df.columns if \"STATISTISCHE\" in c)\n",
    "    df = df.dropna(subset=[seg_col])\n",
    "    df = df[df[seg_col].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "    # post-cleanup\n",
    "    # 1. trim + upper-case every string cell\n",
    "    df = df.applymap(lambda v: v.replace(\"  \", \" \").strip().upper() if isinstance(v, str) else v)\n",
    "\n",
    "    num_cols = cols[2:]\n",
    "    df[num_cols] = (\n",
    "        df[num_cols]\n",
    "          .replace({\"-\": \"0\", \".\": \"0\"})\n",
    "          .astype(str)                       # ensure string for next step\n",
    "    )\n",
    "\n",
    "    df.replace(\"0\", \"\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd3ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layouts of all FZ1 sheets are identical (coordinates, headers, first data row)\n"
     ]
    }
   ],
   "source": [
    "header_map = {\n",
    "    '1':  [\"B9\", \"D9\", \"J8\", \"K9\", \"L9\", \"M9\", \"N9\", \"O9\", \"P9\", \"Q9\", \"R9\", \"S9\", \"T9\", \"U9\"],\n",
    "    '2':  [\"B8\", \"D8\", \"E8\", \"F9\", \"G9\", \"H9\", \"I9\", \"K9\", \"L9\", \"M9\", \"N9\", \"O9\", \"P9\", \"Q9\", \"R9\", \"V9\"],\n",
    "}\n",
    "\n",
    "data_start_row = {'1':10, '2':10}\n",
    "\n",
    "def check_fz1_layout():\n",
    "    issues = []\n",
    "    for num, coords in header_map.items():\n",
    "        ref_names = None        # header texts from the first workbook\n",
    "        ref_file  = None        # its filename (for reference print)\n",
    "\n",
    "        for path in sorted(DATA_DIR.glob(\"fz1_*.xlsx\")):\n",
    "            wb  = load_workbook(path, data_only=True)\n",
    "            sn  = _find_sheet(wb, num)\n",
    "            if not sn:\n",
    "                issues.append(f\"{path.name}: workbook 1.{num} not found\")\n",
    "                continue\n",
    "            \n",
    "            # collect header texts at the expected coordinates\n",
    "            ws = wb[sn]\n",
    "            names = [_clean_header(ws[c].value) for c in coords]\n",
    "\n",
    "            # (1) compare to reference workbook\n",
    "            if ref_names is None:\n",
    "                ref_names, ref_file = names, path.name\n",
    "            elif names != ref_names:\n",
    "                issues.append(f\"{path.name}: 1.{num} – {names} ≠ {ref_names} (reference {ref_file})\")\n",
    "\n",
    "            # (2) make sure the first data row is populated\n",
    "            r0 = data_start_row[num]\n",
    "            if not any(ws[f\"{c[0]}{r0}\"].value for c in coords):\n",
    "                issues.append(f\"{path.name}: 1.{num} – row {r0} is empty, first data row shifted?\")\n",
    "    \n",
    "    # Report\n",
    "    if issues:\n",
    "        print(\"⚠️  Discrepancies have been detected:\")\n",
    "        for msg in issues:\n",
    "            print(\" •\", msg)\n",
    "    else:\n",
    "        print(\"The layouts of all FZ1 sheets are identical (coordinates, headers, first data row)\")\n",
    "\n",
    "# Run the check once:\n",
    "check_fz1_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d486dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_parsers = {'1':  fz1_1, '2':  fz1_2,}\n",
    "\n",
    "# Accumulators: one global DataFrame per sheet number\n",
    "globals_by_sheet = {num: pd.DataFrame() for num in sheet_parsers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c38bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sorted(DATA_DIR.glob(\"fz1_*.xlsx\")):\n",
    "    wb   = load_workbook(path, data_only=True)      # read Excel as values\n",
    "    date = _date_from_fname(path)                   # e.g. \"2024\"\n",
    "\n",
    "    for num, parser in sheet_parsers.items():\n",
    "        sname = _find_sheet(wb, num)                 # locate “FZ 1.<num>”\n",
    "        if not sname:                               # skip missing sheets\n",
    "            print(f\"{path.name}: workbook 1.{num} not found\")\n",
    "            continue\n",
    "\n",
    "        df = parser(wb[sname])                      # parse & clean\n",
    "        df.insert(0, \"Date\", date)                  # add period column\n",
    "\n",
    "        # append to the global accumulator for this sheet\n",
    "        globals_by_sheet[num] = pd.concat([globals_by_sheet[num], df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe346d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Saved fz_1.1_raw.csv  →  (2396, 15)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2396 entries, 0 to 2395\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                        Non-Null Count  Dtype \n",
      "---  ------                                        --------------  ----- \n",
      " 0   Date                                          2396 non-null   object\n",
      " 1   LAND                                          2396 non-null   object\n",
      " 2   STATISTISCHE KENNZIFFER UND ZULASSUNGSBEZIRK  2396 non-null   object\n",
      " 3   PERSONENKRAFTWAGEN                            2396 non-null   object\n",
      " 4   HUBRAUM BIS 1.399 CM³                         2396 non-null   object\n",
      " 5   1.400 BIS 1.999 CM³                           2396 non-null   object\n",
      " 6   2.000 UND MEHR CM³                            2396 non-null   object\n",
      " 7   UNBE- KANNT                                   2396 non-null   object\n",
      " 8   UND ZWAR MIT OFFENEM AUFBAU                   2396 non-null   object\n",
      " 9   UND ZWAR MIT ALLRAD- ANTRIEB                  2396 non-null   object\n",
      " 10  UND ZWAR WOHN- MOBILE                         2396 non-null   object\n",
      " 11  UND ZWAR KRANKEN- WAGEN, NOTARZT- EINSATZFZ.  2396 non-null   object\n",
      " 12  UND ZWAR GEWERBLICHE HALTERINNEN UND HALTER   2396 non-null   object\n",
      " 13  UND ZWAR HALTERINNEN                          2396 non-null   object\n",
      " 14  PKW-DICHTE JE 1.000 EINWOHNER                 2396 non-null   object\n",
      "dtypes: object(15)\n",
      "memory usage: 280.9+ KB\n",
      "\n",
      "\n",
      "\n",
      "• Saved fz_1.2_raw.csv  →  (2396, 17)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2396 entries, 0 to 2395\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                        Non-Null Count  Dtype \n",
      "---  ------                                        --------------  ----- \n",
      " 0   Date                                          2396 non-null   object\n",
      " 1   LAND                                          2396 non-null   object\n",
      " 2   STATISTISCHE KENNZIFFER UND ZULASSUNGSBEZIRK  2396 non-null   object\n",
      " 3   INSGESAMT                                     2396 non-null   object\n",
      " 4   BENZIN                                        2396 non-null   object\n",
      " 5   DIESEL                                        2396 non-null   object\n",
      " 6   GAS (EINSCHL. BIVALENT)                       2396 non-null   object\n",
      " 7   HYBRID INSGESAMT                              2396 non-null   object\n",
      " 8   ELEKTRO (BEV)                                 2396 non-null   object\n",
      " 9   SONSTIGE                                      2396 non-null   object\n",
      " 10  EURO 1                                        2396 non-null   object\n",
      " 11  EURO 2                                        2396 non-null   object\n",
      " 12  EURO 3                                        2396 non-null   object\n",
      " 13  EURO 4                                        2396 non-null   object\n",
      " 14  EURO 5                                        2396 non-null   object\n",
      " 15  EURO 6                                        2396 non-null   object\n",
      " 16  SONSTIGE1                                     2396 non-null   object\n",
      "dtypes: object(17)\n",
      "memory usage: 318.3+ KB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, df in globals_by_sheet.items():\n",
    "    # ensure 100 % string representation, no NaN\n",
    "    df = df.fillna('').astype(str)\n",
    "\n",
    "    # path …/csv/fz_1.<num>_raw.csv\n",
    "    out_csv = OUT_DIR / f\"fz_1.{num}_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    out_csv = DST_DIR / f\"fz_1.{num}_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # console log\n",
    "    print(f\"• Saved {out_csv.name}  →  {df.shape}\\n\")\n",
    "    df.info()           # quick dtype audit\n",
    "    print(\"\\n\\n\")       # visual separator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
