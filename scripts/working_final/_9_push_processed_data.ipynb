{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5346aa12",
   "metadata": {},
   "source": [
    "# PUSH ALL DATA TO DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Upload processed CSV files into PostgreSQL tables.\n",
    "\n",
    "This script will:\n",
    "1. Load database connection parameters from a .env file.\n",
    "2. Create a SQLAlchemy engine.\n",
    "3. Iterate over every CSV in ../data/processed.\n",
    "4. Read each CSV as text-only into a pandas DataFrame.\n",
    "5. Map every column to SQL TEXT type.\n",
    "6. Replace (or create) the corresponding table in the target schema.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ca5f2",
   "metadata": {},
   "source": [
    "## LOAD CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ecca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .env into a dict of strings\n",
    "config    = dotenv_values()\n",
    "\n",
    "# Extract Postgres credentials / connection info\n",
    "pg_user   = config['POSTGRES_USER']\n",
    "pg_pass   = config['POSTGRES_PASS']\n",
    "pg_host   = config['POSTGRES_HOST']\n",
    "pg_port   = config['POSTGRES_PORT']\n",
    "pg_db     = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55223a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SQLAlchemy database URL\n",
    "db_url = f\"postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "\n",
    "# This engine will manage connections & SQL execution\n",
    "engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dab83c",
   "metadata": {},
   "source": [
    "## PROCESS CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR  = Path(\"../../data/processed/,\")\n",
    "FILES     = [\"_handelsnamen_pkw.csv\", \"_modellreihen.csv\"]\n",
    "KEEP_TEXT = 5\n",
    "\n",
    "def _to_float(col: pd.Series) -> pd.Series:\n",
    "    dash_rx = re.compile(r\"^[-\\u2013\\u2014]$\")\n",
    "    col = col.str.strip()\n",
    "    col = col.mask(col.str.match(dash_rx) | (col == \".\"), pd.NA)\n",
    "    col = col.str.replace(r\"\\s|\\.\", \"\", regex=True)\n",
    "    col = col.str.replace(\",\", \".\", regex=False)\n",
    "    return pd.to_numeric(col, errors=\"coerce\")\n",
    "\n",
    "for fname in FILES:\n",
    "    path = DATA_DIR / fname\n",
    "    if not path.exists():\n",
    "        print(f\"!! {fname} not found\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    zs_mask = df.columns.str.contains(r\"ZS\\s|\\sZS\", case=False, regex=True)\n",
    "    if zs_mask.any():\n",
    "        df = df.loc[:, ~zs_mask]\n",
    "\n",
    "    numeric_cols = df.columns[KEEP_TEXT:]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = _to_float(df[col])\n",
    "\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    df.info()\n",
    "\n",
    "print(\"\\nReady.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed27a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk both “;” and “,” subfolders\n",
    "for csv_path in data_dir.glob(\"*/*.csv\"):\n",
    "    # derive table name from filename\n",
    "    table_name = csv_path.stem.lower().replace(\"-\", \"_\")\n",
    "\n",
    "    # determine delimiter from the parent folder name\n",
    "    sep = csv_path.parent.name  # either \";\" or \",\"\n",
    "\n",
    "    # read CSV as text, with the correct delimiter\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        # dtype=str,\n",
    "        sep=sep,\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"warn\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # map every column to TEXT in Postgres\n",
    "    # dtype_dict = {col: Text() for col in df.columns}\n",
    "    \n",
    "    # write (replace) into the target schema\n",
    "    df.to_sql(\n",
    "        name      = table_name,\n",
    "        con       = engine,\n",
    "        schema    = pg_schema,\n",
    "        if_exists = \"replace\",\n",
    "        index     = False,\n",
    "        # dtype     = dtype_dict\n",
    "    )\n",
    "    \n",
    "    print(f\"Uploaded: {pg_schema}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
