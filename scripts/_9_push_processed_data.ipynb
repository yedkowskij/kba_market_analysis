{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5346aa12",
   "metadata": {},
   "source": [
    "# Database Upload Pipeline: Processed Vehicle Registration Data\n",
    "\n",
    "This notebook handles the upload of all processed German vehicle registration CSV \n",
    "files from the analysis pipeline into a PostgreSQL database. The implementation \n",
    "automatically detects CSV files, determines appropriate table structures, and \n",
    "manages database connections through environment configuration.\n",
    "\n",
    "## Workflow Overview\n",
    "1. Load database connection parameters from environment file\n",
    "2. Establish SQLAlchemy connection to PostgreSQL database\n",
    "3. Process CSV files with appropriate delimiters and German character support\n",
    "4. Create or replace database tables with standardized schemas\n",
    "5. Upload data with proper error handling and progress monitoring\n",
    "\n",
    "## Key Variables\n",
    "- `config`: Database connection parameters from .env file\n",
    "- `engine`: SQLAlchemy database engine for connection management\n",
    "- `DATA_DIR`: Directory containing processed CSV files\n",
    "- `pg_schema`: Target PostgreSQL schema for table creation\n",
    "\n",
    "## Prerequisites\n",
    "- .env file must contain valid PostgreSQL connection parameters\n",
    "- Target database and schema must exist and be accessible\n",
    "- CSV files must be properly formatted with UTF-8 encoding in processed data directory\n",
    "- Required Python packages: pandas, SQLAlchemy, python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import essential libraries for database upload operations ===\n",
    "from pathlib import Path           # Modern path handling for cross-platform compatibility\n",
    "from dotenv import dotenv_values   # Environment configuration loading\n",
    "import pandas as pd               # Data manipulation and analysis framework\n",
    "import re                          # Regular expression pattern matching\n",
    "from sqlalchemy import create_engine    # Database connection management\n",
    "from sqlalchemy.types import Text # Database column type specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ca5f2",
   "metadata": {},
   "source": [
    "## Database Configuration\n",
    "\n",
    "Load PostgreSQL connection parameters from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ecca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load environment configuration file as dictionary ===\n",
    "config    = dotenv_values()  # Read .env file into string dictionary\n",
    "\n",
    "# === Extract PostgreSQL connection parameters ===\n",
    "pg_user   = config['POSTGRES_USER']    # Database username credential\n",
    "pg_pass   = config['POSTGRES_PASS']    # Database password credential\n",
    "pg_host   = config['POSTGRES_HOST']    # Database server hostname/IP\n",
    "pg_port   = config['POSTGRES_PORT']    # Database server port number\n",
    "pg_db     = config['POSTGRES_DB']      # Target database name\n",
    "pg_schema = config['POSTGRES_SCHEMA']  # Target schema for table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55223a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build PostgreSQL connection URL for SQLAlchemy ===\n",
    "db_url = f\"postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "\n",
    "# === Create database engine for connection management ===\n",
    "engine = create_engine(db_url)  # SQLAlchemy engine for database operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dab83c",
   "metadata": {},
   "source": [
    "## CSV Processing and Upload\n",
    "\n",
    "Process specific CSV files with numeric conversion and upload to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define data directory and target files for processing ===\n",
    "DATA_DIR  = Path(\"../data/processed/,\")                # Processed data directory\n",
    "FILES     = [\"_handelsnamen_pkw.csv\", \"_modellreihen.csv\"]  # Specific files to process\n",
    "KEEP_TEXT = 5                                          # Number of text columns to preserve\n",
    "\n",
    "def _to_float(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert string column to float with German formatting and dash handling.\n",
    "    \n",
    "    Args:\n",
    "        col (pd.Series): String column containing numeric data\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Numeric column with proper float64 dtype\n",
    "    \"\"\"\n",
    "    # === Compile regex for various dash characters ===\n",
    "    dash_rx = re.compile(r\"^[-\\u2013\\u2014]$\")\n",
    "    # === Remove leading/trailing whitespace ===\n",
    "    col = col.str.strip()\n",
    "    # === Replace dashes and dots with NA ===\n",
    "    col = col.mask(col.str.match(dash_rx) | (col == \".\"), pd.NA)\n",
    "    # === Remove spaces and thousand separators ===\n",
    "    col = col.str.replace(r\"\\s|\\.\", \"\", regex=True)\n",
    "    # === Convert German decimal comma to dot ===\n",
    "    col = col.str.replace(\",\", \".\", regex=False)\n",
    "    # === Convert to numeric, invalid values become NaN ===\n",
    "    return pd.to_numeric(col, errors=\"coerce\")\n",
    "\n",
    "# === Process each target file ===\n",
    "for fname in FILES:\n",
    "    path = DATA_DIR / fname\n",
    "    # === Check if file exists before processing ===\n",
    "    if not path.exists():\n",
    "        print(f\"!! {fname} not found\")\n",
    "        continue\n",
    "\n",
    "    # === Load CSV as string data ===\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    # === Clean column names (remove whitespace) ===\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # === Remove columns containing 'ZS' (special handling) ===\n",
    "    zs_mask = df.columns.str.contains(r\"ZS\\s|\\sZS\", case=False, regex=True)\n",
    "    if zs_mask.any():\n",
    "        df = df.loc[:, ~zs_mask]\n",
    "\n",
    "    # === Convert numeric columns (skip first 5 text columns) ===\n",
    "    numeric_cols = df.columns[KEEP_TEXT:]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = _to_float(df[col])\n",
    "\n",
    "    # === Save processed data back to file ===\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    # === Display DataFrame info for verification ===\n",
    "    df.info()\n",
    "\n",
    "print(\"\\nReady.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006efe35",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Bulk Database Upload\n",
    "\n",
    "Process all CSV files in subdirectories and upload to PostgreSQL database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define base directory for processed CSV files ===\n",
    "data_dir = Path(\"../data/processed/\")  # Directory containing CSV files for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed27a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk both “;” and “,” subfolders\n",
    "# === Walk through subdirectories to find CSV files with different delimiters ===\n",
    "for csv_path in data_dir.glob(\"*/*.csv\"):\n",
    "    # === Derive table name from filename with normalization ===\n",
    "    table_name = csv_path.stem.lower().replace(\"-\", \"_\")  # Convert to SQL-safe naming\n",
    "\n",
    "    # === Determine delimiter from parent directory name ===\n",
    "    sep = csv_path.parent.name  # Directory name indicates delimiter (\";\" or \",\")\n",
    "\n",
    "    # === Read CSV with appropriate delimiter and encoding ===\n",
    "    df = pd.read_csv(\n",
    "        csv_path,                      # File path to process\n",
    "        sep=sep,                       # Use directory-specified delimiter\n",
    "        engine=\"python\",               # Python engine for flexible parsing\n",
    "        on_bad_lines=\"warn\",           # Warn about malformed lines\n",
    "        encoding=\"utf-8\"               # UTF-8 encoding for German characters\n",
    "    )\n",
    "    \n",
    "    # === Upload DataFrame to PostgreSQL database ===\n",
    "    df.to_sql(\n",
    "        name      = table_name,        # SQL table name derived from filename\n",
    "        con       = engine,            # SQLAlchemy database connection\n",
    "        schema    = pg_schema,         # Target schema for table creation\n",
    "        if_exists = \"replace\",         # Replace existing table if present\n",
    "        index     = False              # Don't include DataFrame index as column\n",
    "    )\n",
    "    \n",
    "    # === Confirm successful upload with table identifier ===\n",
    "    print(f\"Uploaded: {pg_schema}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
