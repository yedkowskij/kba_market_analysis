{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab32df8",
   "metadata": {},
   "source": [
    "# FZ10 PREP (2020 - 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81b612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================#\n",
    "#  FZ-10 WORKBOOK INGEST (2020-2025)                                          #\n",
    "#  -------------------------------------------------------------------------  #\n",
    "#  • Reads every “fz10_YYYY_MM.xlsx” in ../data/raw/fz10/                     #\n",
    "#  • Sheet of interest:  **FZ 10.1**  (both “FZ10.1” and “FZ 10.1” accepted)  #\n",
    "#  • 2020 files have a *short* column set (B-S); newer files use B-AQ.        #\n",
    "#  • Cleans, de-dupes, forward-fills “Marke”, flags “SONSTIGE”, builds        #\n",
    "#    composite column “Modellreihe” = Marke + Modell.                         #\n",
    "#  • Saves raw CSV (all cells as text) to ../data/raw/fz10/csv/               #\n",
    "#    and a convenience copy to ../data/processed/                             #\n",
    "# ============================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e1afa",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  IMPORTS & PATHS  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1daf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "DATA_DIR = Path(\"../data/raw/fz10\")             # source workbooks\n",
    "OUT_DIR  = Path(\"../data/raw/fz10/csv\")         # intermediate CSVs\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DST_DIR = Path(\"../data/processed/,\")             # final analytics CSV\n",
    "DST_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9420d6",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  HELPERS  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c60ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _date_from_fname(p):\n",
    "    \"\"\"Return YYYYMM extracted from filename `fz10_YYYY_MM.xlsx`.\"\"\"\n",
    "    y, m = re.search(r\"(\\d{4})_(\\d{2})\", p.name).groups()\n",
    "    return y + m\n",
    "\n",
    "def _clean_header(s):\n",
    "    \"\"\"Normalize header cell: collapse multiple spaces + remove newlines.\"\"\"\n",
    "    return str(s).replace('\\n', ' ').replace('  ', ' ').strip() if s is not None else s\n",
    "\n",
    "def _strip_cols(df):\n",
    "    \"\"\"Apply `clean_header` to every column name in-place and return df.\"\"\"\n",
    "    df.columns = [_clean_header(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique(cols):\n",
    "    \"\"\"Ensure uniqueness by adding numeric suffixes.\"\"\"\n",
    "    seen, out = {}, []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}{seen[c]}\")\n",
    "        else:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "def _col(ws, letter, r0, r1):\n",
    "    \"\"\"Return values of column *letter* from rows *r0 … r1* inclusive.\"\"\"\n",
    "    return [ws[f\"{letter}{row}\"].value for row in range(r0, r1 + 1)]\n",
    "\n",
    "def _find_sheet(wb, pattern=r\"^FZ\\s*10\\.1$\"):\n",
    "    \"\"\"Locate sheet whose name matches *pattern* (case-insensitive).\"\"\"\n",
    "    regex = re.compile(pattern, flags=re.IGNORECASE)\n",
    "    for name in wb.sheetnames:\n",
    "        if regex.match(name.strip()):\n",
    "            return name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd06ffa",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  PARSER  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086c09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fz10_1(ws, is_2020):\n",
    "    \"\"\"\n",
    "    Parse sheet **FZ 10.1**.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ws : openpyxl.worksheet.Worksheet\n",
    "    is_2020 : bool\n",
    "        • True  → use legacy 2020 column map (letters B … S)  \n",
    "        • False → use current map      (letters B … AQ)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Clean table (rows 10-1000) with extra column “Modellreihe”.\n",
    "    \"\"\"\n",
    "\n",
    "    # coordinate maps\n",
    "    letters_2020 = [\"B\", \"C\", \"D\", \"G\", \"J\", \"M\", \"P\", \"S\"]\n",
    "    hdr_map_2020 = [\n",
    "        _clean_header(ws[\"B9\"].value),    # Marke\n",
    "        _clean_header(ws[\"C9\"].value),    # Modellreihe\n",
    "        _clean_header(ws[\"D8\"].value),    # Insgesamt\n",
    "        _clean_header(ws[\"G8\"].value),    # mit Dieselantrieb\n",
    "        _clean_header(ws[\"J8\"].value),    # mit Hybridantrieb (incl. Plug-in-Hybrid)\n",
    "        _clean_header(ws[\"M8\"].value),    # mit Elektroantrieb (BEV)\n",
    "        _clean_header(ws[\"P8\"].value),    # mit Allradantrieb\n",
    "        _clean_header(ws[\"S8\"].value),    # Cabriolets\n",
    "    ]\n",
    "\n",
    "    letters_new = [\"B\", \"C\", \"D\", \"G\", \"J\", \"AK\", \"AN\", \"AQ\"]\n",
    "    hdr_map_new = [\n",
    "        _clean_header(ws[\"B9\"].value),    # Marke\n",
    "        _clean_header(ws[\"C9\"].value),    # Modellreihe\n",
    "        _clean_header(ws[\"D8\"].value),    # Insgesamt\n",
    "        _clean_header(ws[\"G8\"].value),    # mit Dieselantrieb\n",
    "        _clean_header(ws[\"J8\"].value),    # mit Hybridantrieb (incl. Plug-in-Hybrid)\n",
    "        _clean_header(ws[\"AK8\"].value),   # mit Elektroantrieb (BEV)\n",
    "        _clean_header(ws[\"AN8\"].value),   # mit Allradantrieb\n",
    "        _clean_header(ws[\"AQ8\"].value),   # Cabriolets\n",
    "    ]\n",
    "\n",
    "    letters = letters_2020 if is_2020 else letters_new\n",
    "    headers = hdr_map_2020 if is_2020 else hdr_map_new\n",
    "\n",
    "    # remove verbose tail fragments\n",
    "    headers = [h.replace(\"(incl. Plug-in-Hybrid)\", \"\").strip() for h in headers]\n",
    "    headers = [h.replace(\"(BEV)\", \"\").strip() for h in headers]\n",
    "    cols    = _unique(headers)\n",
    "    \n",
    "    # read data block\n",
    "    df = pd.DataFrame({\n",
    "        name: _col(ws, col, 10, 1000)\n",
    "        for name, col in zip(cols, letters)\n",
    "    }).dropna(how=\"all\")\n",
    "\n",
    "    # normalise header texts once more (harmless if already clean)\n",
    "    df = _strip_cols(df)\n",
    "\n",
    "    # rename “Modellreihe” → “Modell”\n",
    "    modell_col = None\n",
    "    for c in list(df.columns):\n",
    "        if \"modellreihe\" in c.lower():\n",
    "            df.rename(columns={c: \"Modell\"}, inplace=True)\n",
    "            modell_col = \"Modell\"\n",
    "\n",
    "    marke_col = next((c for c in df.columns if \"marke\" in c.lower()), df.columns[0])\n",
    "\n",
    "    # drop meta rows\n",
    "    trash = r\"INSGESAMT|ZUSAMMEN|FLENSBURG|ANZAHL|HINWEIS|UMBENANNT\"\n",
    "    df = df[~df[marke_col].astype(str).str.contains(trash, case=False, na=False)].reset_index(drop=True)\n",
    "\n",
    "    # forward-fill Marke + flag SONSTIGE\n",
    "    df[marke_col] = df[marke_col].ffill()\n",
    "    sonstige_mask = df[marke_col].str.contains(r\"\\bSONSTIGE\\b\", case=False, na=False)\n",
    "    if sonstige_mask.any():\n",
    "        next_col = df.columns[df.columns.get_loc(marke_col) + 1]\n",
    "        df.loc[sonstige_mask, next_col] = \"SONSTIGE\"\n",
    "\n",
    "    # insert composite “Modellreihe”\n",
    "    if modell_col:\n",
    "        pos = df.columns.get_loc(modell_col) + 1\n",
    "        df.insert(pos, \"Modellreihe\",\n",
    "                  (df[marke_col].fillna(\"\") + \" \" + df[modell_col].fillna(\"\")).str.strip())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12433650",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  VALIDATION FUNCTION  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220b02e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All FZ-10 workbooks share identical layout per year-class\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "─────────────────────────────  HEADER VALIDATION  ──────────────────────────\n",
    "Two header layouts exist:\n",
    "  • 2020 release  → letters  B-S   (M, P, S instead of AK, AN, AQ)\n",
    "  • 2021-2025     → letters  B-AQ\n",
    "\"\"\"\n",
    "\n",
    "HDR_2020 = [\"B9\", \"C9\", \"D8\", \"G8\", \"J8\", \"M8\", \"P8\", \"S8\"]\n",
    "HDR_NEW  = [\"B9\", \"C9\", \"D8\", \"G8\", \"J8\", \"AK8\", \"AN8\", \"AQ8\"]\n",
    "DATA_FIRST_ROW = 10             # first row of real data for *all* years\n",
    "\n",
    "def _header_coords(path):\n",
    "    \"\"\"\n",
    "    Choose the correct coordinate list for a workbook.\n",
    "\n",
    "    The year is encoded in the filename: e.g. `fz10_2020_05.xlsx`.\n",
    "    If the filename contains “2020” → legacy layout is returned,\n",
    "    otherwise the new (2021-2025) layout.\n",
    "    \"\"\"\n",
    "    return HDR_2020 if \"2020\" in path.name else HDR_NEW\n",
    "\n",
    "def check_fz10_layout():\n",
    "    \"\"\"\n",
    "    Sanity-check *all* FZ-10 workbooks in `DATA_DIR`.\n",
    "\n",
    "    What is verified?\n",
    "    -----------------\n",
    "    1.  **Sheet presence** – every file must contain “FZ 10.1”.  \n",
    "    2.  **Header texts**   – all files *from 2021 onward* must share the\n",
    "        exact same header strings (2020 is ignored because its schema differs).  \n",
    "    3.  **Data anchor**    – cell block starting at row 10 must not be empty\n",
    "        (catches accidental vertical shifts).\n",
    "\n",
    "    This function is called once before the main ingest loop so that\n",
    "    structural problems are surfaced early.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    ref_names = None        # header snapshot from first “new” workbook\n",
    "    ref_file  = None        # file that provided that snapshot\n",
    "\n",
    "    # iterate newest→oldest for a cleaner log\n",
    "    for path in sorted(DATA_DIR.glob(\"fz10_*.xlsx\"), reverse=True):\n",
    "        wb = load_workbook(path, data_only=True)\n",
    "        sn = _find_sheet(wb)\n",
    "        if not sn:\n",
    "            issues.append(f\"{path.name}: sheet FZ 10.1 not found\")\n",
    "            continue\n",
    "        \n",
    "        coords = _header_coords(path)          # choose correct layout\n",
    "        ws     = wb[sn]\n",
    "        names  = [_clean_header(ws[c].value) for c in coords]\n",
    "\n",
    "        # (1) compare header texts with first file in the same *layout class*\n",
    "        if ref_names is None and coords is HDR_NEW:\n",
    "            ref_names, ref_file = names, path.name\n",
    "        elif coords is HDR_NEW and names != ref_names:\n",
    "            issues.append(f\"{path.name}: headers {names} ≠ {ref_names} (ref {ref_file})\")\n",
    "\n",
    "        # (2) verify first data row is filled\n",
    "        if not any(ws[f\"{c[0]}{DATA_FIRST_ROW}\"].value for c in coords):\n",
    "            issues.append(f\"{path.name}: row {DATA_FIRST_ROW} is empty — data block shifted?\")\n",
    "\n",
    "    # Report\n",
    "    if issues:\n",
    "        print(\"⚠️ Discrepancies detected:\")\n",
    "        for m in issues:\n",
    "            print(\" •\", m)\n",
    "    else:\n",
    "        print(\"✓ All FZ-10 workbooks share identical layout per year-class\")\n",
    "\n",
    "# Run the check once:\n",
    "check_fz10_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d0b4a",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  DICT WORKBOOK → PARSER  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df3c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---------------------------------------------------------------------------\n",
    " DISPATCH TABLE & ACCUMULATORS\n",
    " --------------------------------------------------------------------------\n",
    " • `sheet_parsers` keeps a mapping  {sheet_id → parser_function}.\n",
    "   Right now we only care about sheet “FZ 10.1”, hence one entry.\n",
    "   If later you need “FZ 10.2” etc. just add:\n",
    "       sheet_parsers[\"2\"] = fz10_2\n",
    "\n",
    " • `globals_by_sheet` is a dict of *empty* DataFrames, one per sheet-id.\n",
    "   During the main loop we keep appending monthly chunks to the correct\n",
    "   DataFrame, so at the end `globals_by_sheet[\"1\"]` holds the full\n",
    "   2020-2025 history for sheet 10.1.\n",
    "---------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "sheet_parsers = {\"1\": fz10_1}\n",
    "\n",
    "# Accumulators: one global DataFrame per sheet number\n",
    "globals_by_sheet = {n: pd.DataFrame() for n in sheet_parsers}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f3c7b",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  MAIN LOOP  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6aa3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "Ingest loop ─ read every monthly **FZ-10** workbook and append to one table\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "Workflow\n",
    "========\n",
    "1.  Iterate over every file that matches  `fz10_YYYY_MM.xlsx`\n",
    "    (sorted in **reverse** order so the console log starts with\n",
    "    the most recent month).\n",
    "\n",
    "2.  For each workbook:\n",
    "      • Load via *openpyxl* with  `data_only=True`  → we get pure values  \n",
    "        (no formula objects).  \n",
    "      • Derive the period tag **YYYYMM** from the filename – this becomes\n",
    "        the first column **Date** in every parsed row.  \n",
    "      • Decide whether the file is a **2020-layout** book  \n",
    "        (keyword “2020” in the filename) – the parser needs this flag.\n",
    "\n",
    "3.  Locate the worksheet *FZ 10.1* using `_find_sheet()`; the helper\n",
    "    accepts both spelling variants “FZ10.1” and “FZ 10.1”.\n",
    "      • If the sheet is missing: emit a warning and skip the file.  \n",
    "      • Otherwise:  \n",
    "          ▸ call `fz10_1(ws, is_2020=…)` to obtain a cleaned `DataFrame`  \n",
    "          ▸ insert the **Date** column at position 0  \n",
    "          ▸ concatenate the rows onto the single accumulator\n",
    "            `globals_by_sheet[\"1\"]` (ignore the old index).\n",
    "\n",
    "Outcome\n",
    "-------\n",
    "When the loop completes, `globals_by_sheet[\"1\"]` contains **every row from\n",
    "every workbook (2020-2025) of sheet FZ 10.1** – ready for CSV export or\n",
    "further analysis.\n",
    "──────────────────────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "\n",
    "for path in sorted(DATA_DIR.glob(\"fz10_*.xlsx\"), reverse=True):\n",
    "    wb   = load_workbook(path, data_only=True)      # open workbook\n",
    "    date = _date_from_fname(path)                   # e.g. \"202403\"\n",
    "\n",
    "    is_2020 = \"2020\" in path.name                   # old vs new layout\n",
    "\n",
    "    sheet = _find_sheet(wb)                         # find FZ 10.1\n",
    "    if not sheet:                                   # skip if absent\n",
    "        print(f\"{path.name}: sheet FZ 10.1 not found — skipped\")\n",
    "        continue\n",
    "\n",
    "    df = fz10_1(wb[sheet], is_2020)                 # parse sheet\n",
    "    df.insert(0, \"Date\", date)                      # add date column\n",
    "    \n",
    "    # accumulate in dict entry \"1\"\n",
    "    globals_by_sheet[\"1\"] = pd.concat([globals_by_sheet[\"1\"], df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fc578",
   "metadata": {},
   "source": [
    "## ─────────────────────────────  SAVE RAW CSVs  ─────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a446534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Saved fz_10.1_raw.csv  →  (22682, 10)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22682 entries, 0 to 22681\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Date                22682 non-null  object\n",
      " 1   Marke               22682 non-null  object\n",
      " 2   Modell              22682 non-null  object\n",
      " 3   Modellreihe         22682 non-null  object\n",
      " 4   Insgesamt           22682 non-null  object\n",
      " 5   mit Dieselantrieb   22682 non-null  object\n",
      " 6   mit Hybridantrieb   22682 non-null  object\n",
      " 7   mit Elektroantrieb  22682 non-null  object\n",
      " 8   mit Allradantrieb   22682 non-null  object\n",
      " 9   Cabriolets          22682 non-null  object\n",
      "dtypes: object(10)\n",
      "memory usage: 1.7+ MB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, df in globals_by_sheet.items():\n",
    "    # ensure 100 % string representation, no NaN\n",
    "    df = df.fillna('').astype(str)\n",
    "\n",
    "    # path …/csv/fz_10.<num>_raw.csv\n",
    "    out_csv = OUT_DIR / f\"fz_10.{num}_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    out_csv = DST_DIR / f\"fz_10.{num}_raw.csv\"\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # console log\n",
    "    print(f\"• Saved {out_csv.name}  →  {df.shape}\\n\")\n",
    "    df.info()           # quick dtype audit\n",
    "    print(\"\\n\\n\")       # visual separator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
